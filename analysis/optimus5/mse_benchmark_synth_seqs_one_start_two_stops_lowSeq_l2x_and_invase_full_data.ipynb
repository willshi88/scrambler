{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#12/29/20\n",
    "#runnign synthetic benchmark graphs for synthetic OR datasets generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#making benchmark images \n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Conv1D, MaxPooling1D, LSTM, ConvLSTM2D, GRU, CuDNNLSTM, CuDNNGRU, BatchNormalization, LocallyConnected2D, Permute, TimeDistributed, Bidirectional\n",
    "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from keras.layers.merge import _Merge\n",
    "import keras.losses\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import isolearn.keras as iso\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import isolearn.io as isoio\n",
    "import isolearn.keras as isol\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as spio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sequence_logo_helper import dna_letter_at, plot_dna_logo\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "def contain_tf_gpu_mem_usage() :\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)\n",
    "\n",
    "contain_tf_gpu_mem_usage()\n",
    "\n",
    "class EpochVariableCallback(Callback) :\n",
    "    \n",
    "    def __init__(self, my_variable, my_func) :\n",
    "        self.my_variable = my_variable       \n",
    "        self.my_func = my_func\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs={}) :\n",
    "        K.set_value(self.my_variable, self.my_func(K.get_value(self.my_variable), epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ONLY RUN THIS CELL ONCE \n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "#Stochastic Binarized Neuron helper functions (Tensorflow)\n",
    "#ST Estimator code adopted from https://r2rt.com/beyond-binary-ternary-and-one-hot-neurons.html\n",
    "#See Github https://github.com/spitis/\n",
    "\n",
    "def st_sampled_softmax(logits):\n",
    "    with ops.name_scope(\"STSampledSoftmax\") as namescope :\n",
    "        nt_probs = tf.nn.softmax(logits)\n",
    "        onehot_dim = logits.get_shape().as_list()[1]\n",
    "        sampled_onehot = tf.one_hot(tf.squeeze(tf.multinomial(logits, 1), 1), onehot_dim, 1.0, 0.0)\n",
    "        with tf.get_default_graph().gradient_override_map({'Ceil': 'Identity', 'Mul': 'STMul'}):\n",
    "            return tf.ceil(sampled_onehot * nt_probs)\n",
    "\n",
    "def st_hardmax_softmax(logits):\n",
    "    with ops.name_scope(\"STHardmaxSoftmax\") as namescope :\n",
    "        nt_probs = tf.nn.softmax(logits)\n",
    "        onehot_dim = logits.get_shape().as_list()[1]\n",
    "        sampled_onehot = tf.one_hot(tf.argmax(nt_probs, 1), onehot_dim, 1.0, 0.0)\n",
    "        with tf.get_default_graph().gradient_override_map({'Ceil': 'Identity', 'Mul': 'STMul'}):\n",
    "            return tf.ceil(sampled_onehot * nt_probs)\n",
    "\n",
    "@ops.RegisterGradient(\"STMul\")\n",
    "def st_mul(op, grad):\n",
    "    return [grad, grad]\n",
    "\n",
    "#Gumbel Distribution Sampler\n",
    "def gumbel_softmax(logits, temperature=0.5) :\n",
    "    gumbel_dist = tf.contrib.distributions.RelaxedOneHotCategorical(temperature, logits=logits)\n",
    "    batch_dim = logits.get_shape().as_list()[0]\n",
    "    onehot_dim = logits.get_shape().as_list()[1]\n",
    "    return gumbel_dist.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model functions for loading optimus scramblers \n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def mask_dropout_multi_scale(mask, drop_scales=[1, 2, 4, 7], min_drop_rate=0.0, max_drop_rate=0.5) :\n",
    "    rates = K.random_uniform(shape=(K.shape(mask)[0], 1, 1, 1), minval=min_drop_rate, maxval=max_drop_rate)\n",
    "    scale_logits = K.random_uniform(shape=(K.shape(mask)[0], len(drop_scales), 1, 1, 1), minval=-5., maxval=5.)\n",
    "    scale_probs = K.softmax(scale_logits, axis=1)\n",
    "    ret_mask = mask\n",
    "    for drop_scale_ix, drop_scale in enumerate(drop_scales) :\n",
    "        ret_mask = mask_dropout(ret_mask, rates * scale_probs[:, drop_scale_ix, ...], drop_scale=drop_scale)\n",
    "    return K.switch(K.learning_phase(), ret_mask, mask)\n",
    "def mask_dropout(mask, drop_rates, drop_scale=1) :\n",
    "    random_tensor_downsampled = K.random_uniform(shape=(\n",
    "        K.shape(mask)[0],\n",
    "        1,\n",
    "        K.cast(K.shape(mask)[2] / drop_scale, dtype=tf.int32),\n",
    "        K.shape(mask)[3]\n",
    "    ), minval=0.0, maxval=1.0)\n",
    "    keep_mask_downsampled = random_tensor_downsampled >= drop_rates\n",
    "    keep_mask = K.repeat_elements(keep_mask_downsampled, rep=drop_scale, axis=2)\n",
    "    ret_mask = mask * K.cast(keep_mask, dtype=tf.float32)\n",
    "    return ret_mask\n",
    "def mask_dropout_single_scale(mask, drop_scale=1, min_drop_rate=0.0, max_drop_rate=0.5) :\n",
    "    rates = K.random_uniform(shape=(K.shape(mask)[0], 1, 1, 1), minval=min_drop_rate, maxval=max_drop_rate)\n",
    "    random_tensor_downsampled = K.random_uniform(shape=(\n",
    "        K.shape(mask)[0],\n",
    "        1,\n",
    "        K.cast(K.shape(mask)[2] / drop_scale, dtype=tf.int32),\n",
    "        K.shape(mask)[3]\n",
    "    ), minval=0.0, maxval=1.0)\n",
    "    keep_mask_downsampled = random_tensor_downsampled >= rates\n",
    "    keep_mask = K.repeat_elements(keep_mask_downsampled, rep=drop_scale, axis=2)\n",
    "    ret_mask = mask * K.cast(keep_mask, dtype=tf.float32)\n",
    "    return K.switch(K.learning_phase(), ret_mask, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PWM Masking and Sampling helper functions\n",
    "\n",
    "def mask_pwm(inputs) :\n",
    "    pwm, onehot_template, onehot_mask = inputs\n",
    "\n",
    "    return pwm * onehot_mask + onehot_template\n",
    "\n",
    "def sample_pwm_st(pwm_logits) :\n",
    "    n_sequences = K.shape(pwm_logits)[0]\n",
    "    seq_length = K.shape(pwm_logits)[2]\n",
    "\n",
    "    flat_pwm = K.reshape(pwm_logits, (n_sequences * seq_length, 4))\n",
    "    sampled_pwm = st_sampled_softmax(flat_pwm)\n",
    "\n",
    "    return K.reshape(sampled_pwm, (n_sequences, 1, seq_length, 4))\n",
    "\n",
    "def sample_pwm_gumbel(pwm_logits) :\n",
    "    n_sequences = K.shape(pwm_logits)[0]\n",
    "    seq_length = K.shape(pwm_logits)[2]\n",
    "\n",
    "    flat_pwm = K.reshape(pwm_logits, (n_sequences * seq_length, 4))\n",
    "    sampled_pwm = gumbel_softmax(flat_pwm, temperature=0.5)\n",
    "\n",
    "    return K.reshape(sampled_pwm, (n_sequences, 1, seq_length, 4))\n",
    "\n",
    "#Generator helper functions\n",
    "def initialize_sequence_templates(generator, sequence_templates, background_matrices) :\n",
    "\n",
    "    embedding_templates = []\n",
    "    embedding_masks = []\n",
    "    embedding_backgrounds = []\n",
    "\n",
    "    for k in range(len(sequence_templates)) :\n",
    "        sequence_template = sequence_templates[k]\n",
    "        onehot_template = iso.OneHotEncoder(seq_length=len(sequence_template))(sequence_template).reshape((1, len(sequence_template), 4))\n",
    "\n",
    "        for j in range(len(sequence_template)) :\n",
    "            if sequence_template[j] not in ['N', 'X'] :\n",
    "                nt_ix = np.argmax(onehot_template[0, j, :])\n",
    "                onehot_template[:, j, :] = -4.0\n",
    "                onehot_template[:, j, nt_ix] = 10.0\n",
    "            elif sequence_template[j] == 'X' :\n",
    "                onehot_template[:, j, :] = -1.0\n",
    "\n",
    "        onehot_mask = np.zeros((1, len(sequence_template), 4))\n",
    "        for j in range(len(sequence_template)) :\n",
    "            if sequence_template[j] == 'N' :\n",
    "                onehot_mask[:, j, :] = 1.0\n",
    "\n",
    "        embedding_templates.append(onehot_template.reshape(1, -1))\n",
    "        embedding_masks.append(onehot_mask.reshape(1, -1))\n",
    "        embedding_backgrounds.append(background_matrices[k].reshape(1, -1))\n",
    "\n",
    "    embedding_templates = np.concatenate(embedding_templates, axis=0)\n",
    "    embedding_masks = np.concatenate(embedding_masks, axis=0)\n",
    "    embedding_backgrounds = np.concatenate(embedding_backgrounds, axis=0)\n",
    "\n",
    "    generator.get_layer('template_dense').set_weights([embedding_templates])\n",
    "    generator.get_layer('template_dense').trainable = False\n",
    "\n",
    "    generator.get_layer('mask_dense').set_weights([embedding_masks])\n",
    "    generator.get_layer('mask_dense').trainable = False\n",
    "    \n",
    "    generator.get_layer('background_dense').set_weights([embedding_backgrounds])\n",
    "    generator.get_layer('background_dense').trainable = False\n",
    "\n",
    "#Generator construction function\n",
    "def build_sampler(batch_size, seq_length, n_classes=1, n_samples=1, sample_mode='st') :\n",
    "\n",
    "    #Initialize Reshape layer\n",
    "    reshape_layer = Reshape((1, seq_length, 4))\n",
    "    \n",
    "    #Initialize background matrix\n",
    "    onehot_background_dense = Embedding(n_classes, seq_length * 4, embeddings_initializer='zeros', name='background_dense')\n",
    "\n",
    "    #Initialize template and mask matrices\n",
    "    onehot_template_dense = Embedding(n_classes, seq_length * 4, embeddings_initializer='zeros', name='template_dense')\n",
    "    onehot_mask_dense = Embedding(n_classes, seq_length * 4, embeddings_initializer='ones', name='mask_dense')\n",
    "\n",
    "    #Initialize Templating and Masking Lambda layer\n",
    "    masking_layer = Lambda(mask_pwm, output_shape = (1, seq_length, 4), name='masking_layer')\n",
    "    background_layer = Lambda(lambda x: x[0] + x[1], name='background_layer')\n",
    "    \n",
    "    #Initialize PWM normalization layer\n",
    "    pwm_layer = Softmax(axis=-1, name='pwm')\n",
    "    \n",
    "    #Initialize sampling layers\n",
    "    sample_func = None\n",
    "    if sample_mode == 'st' :\n",
    "        sample_func = sample_pwm_st\n",
    "    elif sample_mode == 'gumbel' :\n",
    "        sample_func = sample_pwm_gumbel\n",
    "    \n",
    "    upsampling_layer = Lambda(lambda x: K.tile(x, [n_samples, 1, 1, 1]), name='upsampling_layer')\n",
    "    sampling_layer = Lambda(sample_func, name='pwm_sampler')\n",
    "    permute_layer = Lambda(lambda x: K.permute_dimensions(K.reshape(x, (n_samples, batch_size, 1, seq_length, 4)), (1, 0, 2, 3, 4)), name='permute_layer')\n",
    "    \n",
    "    def _sampler_func(class_input, raw_logits) :\n",
    "        \n",
    "        #Get Template and Mask\n",
    "        onehot_background = reshape_layer(onehot_background_dense(class_input))\n",
    "        onehot_template = reshape_layer(onehot_template_dense(class_input))\n",
    "        onehot_mask = reshape_layer(onehot_mask_dense(class_input))\n",
    "        \n",
    "        #Add Template and Multiply Mask\n",
    "        pwm_logits = masking_layer([background_layer([raw_logits, onehot_background]), onehot_template, onehot_mask])\n",
    "        \n",
    "        #Compute PWM (Nucleotide-wise Softmax)\n",
    "        pwm = pwm_layer(pwm_logits)\n",
    "        \n",
    "        #Tile each PWM to sample from and create sample axis\n",
    "        pwm_logits_upsampled = upsampling_layer(pwm_logits)\n",
    "        sampled_pwm = sampling_layer(pwm_logits_upsampled)\n",
    "        sampled_pwm = permute_layer(sampled_pwm)\n",
    "\n",
    "        sampled_mask = permute_layer(upsampling_layer(onehot_mask))\n",
    "        \n",
    "        return pwm_logits, pwm, sampled_pwm, onehot_mask, sampled_mask\n",
    "    \n",
    "    return _sampler_func\n",
    "\n",
    "#for formulation 2 graphing \n",
    "def returnXMeanLogits(e_train):\n",
    "    #returns x mean logits for displayign the pwm difference for the version 2 networks \n",
    "    #Visualize background sequence distribution\n",
    "    seq_e_train = one_hot_encode(e_train,seq_len=50)\n",
    "    x_train = seq_e_train\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1], x_train.shape[2]))\n",
    "\n",
    "    pseudo_count = 1.0\n",
    "\n",
    "    x_mean = (np.sum(x_train, axis=(0, 1)) + pseudo_count) / (x_train.shape[0] + 4. * pseudo_count)\n",
    "    x_mean_logits = np.log(x_mean / (1. - x_mean))\n",
    "    return x_mean_logits, x_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimus5_synthetic_random_insert_if_uorf_1_start_2_stop_variable_loc_512\n",
      "(512, 1, 50, 4)\n"
     ]
    }
   ],
   "source": [
    "#loading testing dataset \n",
    "\n",
    "from optimusFunctions import *\n",
    "import pandas as pd\n",
    "\n",
    "csv_to_open = \"optimus5_synthetic_random_insert_if_uorf_1_start_2_stop_variable_loc_512.csv\"\n",
    "\n",
    "dataset_name = csv_to_open.replace(\".csv\", \"\")\n",
    "print (dataset_name)\n",
    "data_df = pd.read_csv(\"./\" + csv_to_open) #open from scores folder \n",
    "\n",
    "seq_e_test = one_hot_encode(data_df, seq_len=50)\n",
    "benchmarkSet_seqs = seq_e_test\n",
    "x_test = np.reshape(benchmarkSet_seqs, (benchmarkSet_seqs.shape[0], 1, benchmarkSet_seqs.shape[1], benchmarkSet_seqs.shape[2]))\n",
    "print (x_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training:  15008  testing:  512\n"
     ]
    }
   ],
   "source": [
    "e_train = pd.read_csv(\"bottom5KIFuAUGTop5KIFuAUG.csv\")\n",
    "print (\"training: \", e_train.shape[0], \" testing: \", x_test.shape[0])\n",
    "seq_e_train = one_hot_encode(e_train,seq_len=50)\n",
    "x_mean_logits, x_mean = returnXMeanLogits(e_train)\n",
    "seq_e_train = one_hot_encode(e_train,seq_len=50)\n",
    "x_train = seq_e_train\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1], x_train.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training:  15008  testing:  512\n"
     ]
    }
   ],
   "source": [
    "#background \n",
    "\n",
    "def returnXMeanLogits(e_train):\n",
    "    #returns x mean logits for displayign the pwm difference for the version 2 networks \n",
    "    #Visualize background sequence distribution\n",
    "    seq_e_train = one_hot_encode(e_train,seq_len=50)\n",
    "    x_train = seq_e_train\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1], x_train.shape[2]))\n",
    "    pseudo_count = 1.0\n",
    "    x_mean = (np.sum(x_train, axis=(0, 1)) + pseudo_count) / (x_train.shape[0] + 4. * pseudo_count)\n",
    "    x_mean_logits = np.log(x_mean / (1. - x_mean))\n",
    "    return x_mean_logits, x_mean\n",
    "\n",
    "\n",
    "e_train = pd.read_csv(\"bottom5KIFuAUGTop5KIFuAUG.csv\")\n",
    "print (\"training: \", e_train.shape[0], \" testing: \", x_test.shape[0])\n",
    "\n",
    "#one hot encode with optimus encoders \n",
    "seq_e_train = one_hot_encode(e_train,seq_len=50)\n",
    "x_mean_logits, x_mean = returnXMeanLogits(e_train)\n",
    "x_train = seq_e_train\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1], x_train.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+gAAAAnCAYAAACc9WIYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAAXhJREFUeJzt3LFKQ0EURdEzYm2plY3xQ/z/ysLCLj9gLTZyLZJYiGCCSA6yFrxq7jDTbgbempkAAAAA53Vx7gsAAAAAAh0AAAAqCHQAAAAoINABAACggEAHAACAAgIdAAAACgh0AAAAKCDQAQAAoIBABwAAgAICHQAAAAoIdAAAACgg0AEAAKCAQAcAAIACAh0AAAAKCHQAAAAoINABAACggEAHAACAAgIdAAAACgh0AAAAKCDQAQAAoIBABwAAgAICHQAAAAoIdAAAACgg0AEAAKDA5akb1somyUOSuyQ3Sa6SvCV5SvKS5DXJe5LZf59b9+c9z+TxV7cGAACAf+bkQE9ym+Q+ySbJdXaBvs0uwNeX2UOgf7cGAAAA7K2Z+XnqMLzW8cMAAABAZuaoB+uTAh0AAAD4G34SBwAAAAUEOgAAABQQ6AAAAFBAoAMAAEABgQ4AAAAFBDoAAAAUEOgAAABQQKADAABAAYEOAAAABQQ6AAAAFPgAbx8asfW9BQIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1008x46.8 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean conservation (bits) = 0.032049298346210106\n",
      "Mean KL Div against background (bits) = 1.9679329305814974\n"
     ]
    }
   ],
   "source": [
    "#Define sequence template for optimus\n",
    "\n",
    "sequence_template = 'N'*50\n",
    "sequence_mask = np.array([1 if sequence_template[j] == 'N' else 0 for j in range(len(sequence_template))])\n",
    "\n",
    "#Visualize background sequence distribution\n",
    "\n",
    "save_figs = True\n",
    "plot_dna_logo(np.copy(x_mean), sequence_template=sequence_template, figsize=(14, 0.65), logo_height=1.0, plot_start=0, plot_end=50)\n",
    "\n",
    "#Calculate mean training set conservation\n",
    "\n",
    "entropy = np.sum(x_mean * -np.log(x_mean), axis=-1) / np.log(2.0)\n",
    "conservation = 2.0 - entropy\n",
    "x_mean_conservation = np.sum(conservation) / np.sum(sequence_mask)\n",
    "print(\"Mean conservation (bits) = \" + str(x_mean_conservation))\n",
    "\n",
    "#Calculate mean training set kl-divergence against background\n",
    "x_train_clipped = np.clip(np.copy(x_train[:, 0, :, :]), 1e-8, 1. - 1e-8)\n",
    "kl_divs = np.sum(x_train_clipped * np.log(x_train_clipped / np.tile(np.expand_dims(x_mean, axis=0), (x_train_clipped.shape[0], 1, 1))), axis=-1) / np.log(2.0)\n",
    "x_mean_kl_divs = np.sum(kl_divs * sequence_mask, axis=-1) / np.sum(sequence_mask)\n",
    "x_mean_kl_div = np.mean(x_mean_kl_divs)\n",
    "print(\"Mean KL Div against background (bits) = \" + str(x_mean_kl_div))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initialize Encoder and Decoder networks\n",
    "batch_size = 32\n",
    "seq_length = 50\n",
    "n_samples = 128\n",
    "sample_mode = 'st'\n",
    "#sample_mode = 'gumbel'\n",
    "\n",
    "#Load sampler\n",
    "sampler = build_sampler(batch_size, seq_length, n_classes=1, n_samples=n_samples, sample_mode=sample_mode)\n",
    "\n",
    "#Load Predictor\n",
    "predictor_path = 'optimusRetrainedMain.hdf5'\n",
    "predictor = load_model(predictor_path)\n",
    "predictor.trainable = False\n",
    "predictor.compile(optimizer=keras.optimizers.SGD(lr=0.1), loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Build scrambler model\n",
    "dummy_class = Input(shape=(1,), name='dummy_class')\n",
    "input_logits = Input(shape=(1, seq_length, 4), name='input_logits')\n",
    "\n",
    "pwm_logits, pwm, sampled_pwm, pwm_mask, sampled_mask = sampler(dummy_class, input_logits)\n",
    "\n",
    "scrambler_model = Model([input_logits, dummy_class], [pwm_logits, pwm, sampled_pwm, pwm_mask, sampled_mask])\n",
    "\n",
    "#Initialize Sequence Templates and Masks\n",
    "initialize_sequence_templates(scrambler_model, [sequence_template], [x_mean_logits])\n",
    "\n",
    "scrambler_model.trainable = False\n",
    "scrambler_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999),\n",
    "    loss='mean_squared_error'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "#open all score and reshape as needed \n",
    "\n",
    "file_names = [\n",
    "    \"l2x_\" + dataset_name +  \"_importance_scores_test.npy\",\n",
    "    \"invase_\" + dataset_name +  \"_conv_importance_scores_test.npy\",\n",
    "    \"l2x_\" + dataset_name +  \"_full_data_importance_scores_test.npy\",\n",
    "    \"invase_\" + dataset_name +  \"_conv_full_data_importance_scores_test.npy\",\n",
    "]\n",
    "#deepexplain_optimus_utr_OR_logic_synth_1_start_2_stops_method_integrated_gradients_importance_scores_test.npy\n",
    "\n",
    "model_names =[\n",
    "    \"l2x\",\n",
    "    \"invase\",\n",
    "    \"l2x_full_data\",\n",
    "    \"invase_full_data\",\n",
    "]\n",
    "\n",
    "model_importance_scores_test = [np.load(\"./\" + file_name) for file_name in file_names]\n",
    "\n",
    "for scores in model_importance_scores_test:\n",
    "    print (scores.shape)\n",
    "\n",
    "for model_i in range(len(model_names)) :\n",
    "    if model_importance_scores_test[model_i].shape[-1] > 1 :\n",
    "        model_importance_scores_test[model_i] = np.sum(model_importance_scores_test[model_i], axis=-1, keepdims=True)\n",
    "\n",
    "for scores in model_importance_scores_test:\n",
    "    print (scores.shape)\n",
    "    \n",
    "#reshape for mse script -> if not (3008, 1, 50, 1) make it that shape \n",
    "idealShape = model_importance_scores_test[0].shape\n",
    "print (idealShape)\n",
    "\n",
    "for model_i in range(len(model_names)) :\n",
    "    if model_importance_scores_test[model_i].shape != idealShape:\n",
    "        model_importance_scores_test[model_i] = np.expand_dims(model_importance_scores_test[model_i], 1)\n",
    "        \n",
    "for scores in model_importance_scores_test:\n",
    "    print (scores.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 1, 50, 4)\n",
      "(512, 1, 50, 4)\n",
      "(512, 1)\n",
      "512/512 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "on_state_logit_val = 50.\n",
    "print (x_test.shape)\n",
    "\n",
    "dummy_test = np.zeros((x_test.shape[0], 1))\n",
    "x_test_logits = 2. * x_test - 1.\n",
    "\n",
    "print (x_test_logits.shape)\n",
    "print (dummy_test.shape)\n",
    "\n",
    "\n",
    "x_test_squeezed = np.squeeze(x_test)\n",
    "y_pred_ref = predictor.predict([x_test_squeezed], batch_size=32, verbose=True)[0]\n",
    "\n",
    "_, _, _, pwm_mask, sampled_mask = scrambler_model.predict([x_test_logits, dummy_test], batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'utr', 'gt', 'orig', 'l2x_0_76_quantile_MSE',\n",
      "       'l2x_0_82_quantile_MSE', 'l2x_0_88_quantile_MSE',\n",
      "       'invase_0_76_quantile_MSE', 'invase_0_82_quantile_MSE',\n",
      "       'invase_0_88_quantile_MSE', 'l2x_full_data_0_76_quantile_MSE',\n",
      "       'l2x_full_data_0_82_quantile_MSE', 'l2x_full_data_0_88_quantile_MSE',\n",
      "       'invase_full_data_0_76_quantile_MSE',\n",
      "       'invase_full_data_0_82_quantile_MSE',\n",
      "       'invase_full_data_0_88_quantile_MSE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "feature_quantiles = [0.76, 0.82, 0.88]\n",
    "\n",
    "for name in model_names:\n",
    "    for quantile in feature_quantiles:\n",
    "        totalName = name + \"_\" + str(quantile).replace(\".\",\"_\") + \"_quantile_MSE\"\n",
    "        data_df[totalName] = None\n",
    "    \n",
    "print (data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking model 'l2x'...\n",
      "Feature quantile = 0.76\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.82\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.88\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Benchmarking model 'invase'...\n",
      "Feature quantile = 0.76\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.82\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.88\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Benchmarking model 'l2x_full_data'...\n",
      "Feature quantile = 0.76\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.82\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.88\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Benchmarking model 'invase_full_data'...\n",
      "Feature quantile = 0.76\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.82\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.88\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n"
     ]
    }
   ],
   "source": [
    "feature_quantiles = [0.76, 0.82, 0.88]\n",
    "\n",
    "#batch_size = 128 \n",
    "from sklearn import metrics\n",
    "model_mses = []\n",
    "for model_i in range(len(model_names)) :\n",
    "    \n",
    "    print(\"Benchmarking model '\" + str(model_names[model_i]) + \"'...\")\n",
    "    \n",
    "    feature_quantile_mses = []\n",
    "    \n",
    "    for feature_quantile_i, feature_quantile in enumerate(feature_quantiles) :\n",
    "        \n",
    "        print(\"Feature quantile = \" + str(feature_quantile))\n",
    "    \n",
    "        if len(model_importance_scores_test[model_i].shape) >= 5 :\n",
    "            importance_scores_test = np.abs(model_importance_scores_test[model_i][feature_quantile_i, ...])\n",
    "        else :\n",
    "            importance_scores_test = np.abs(model_importance_scores_test[model_i])\n",
    "        \n",
    "        n_to_test = importance_scores_test.shape[0] // batch_size * batch_size\n",
    "        importance_scores_test = importance_scores_test[:n_to_test]\n",
    "        \n",
    "        importance_scores_test *= np.expand_dims(np.max(pwm_mask[:n_to_test], axis=-1), axis=-1)\n",
    "\n",
    "        quantile_vals = np.quantile(importance_scores_test, axis=(1, 2, 3), q=feature_quantile, keepdims=True)\n",
    "        quantile_vals = np.tile(quantile_vals, (1, importance_scores_test.shape[1], importance_scores_test.shape[2], importance_scores_test.shape[3]))\n",
    "\n",
    "        top_logits_test = np.zeros(importance_scores_test.shape)\n",
    "        top_logits_test[importance_scores_test > quantile_vals] = on_state_logit_val\n",
    "        \n",
    "        top_logits_test = np.tile(top_logits_test, (1, 1, 1, 4)) * x_test_logits[:n_to_test]\n",
    "\n",
    "        _, _, samples_test, _, _ = scrambler_model.predict([top_logits_test, dummy_test[:n_to_test]], batch_size=batch_size)\n",
    "        print (samples_test.shape)\n",
    "        msesPerPoint = []\n",
    "        for data_ix in range(samples_test.shape[0]) :\n",
    "            #for each sample, look at kl divergence for the 128 size batch generated \n",
    "            #for MSE, just track the pred vs original pred \n",
    "            if data_ix % 1000 == 0 :\n",
    "                print(\"Processing example \" + str(data_ix) + \"...\")\n",
    "            \n",
    "            #from optimus R^2, MSE, Pearson R script \n",
    "            justPred = np.expand_dims(np.expand_dims(x_test[data_ix, 0, :, :], axis=0), axis=-1)\n",
    "            justPredReshape = np.reshape(justPred, (1,50,4))\n",
    "            \n",
    "            expanded = np.expand_dims(samples_test[data_ix, :, 0, :, :], axis=-1) #batch size is 128 \n",
    "            expandedReshape = np.reshape(expanded, (n_samples, 50,4))\n",
    "            \n",
    "            y_test_hat_ref = predictor.predict(x=justPredReshape, batch_size=1)[0][0]\n",
    "            \n",
    "            y_test_hat = predictor.predict(x=[expandedReshape], batch_size=32)\n",
    "            \n",
    "            pwmGenerated = y_test_hat.tolist()\n",
    "            tempOriginals = [y_test_hat_ref]*y_test_hat.shape[0]\n",
    "            \n",
    "            asArrayOrig = np.array(tempOriginals)\n",
    "            asArrayGen = np.array(pwmGenerated)\n",
    "            squeezed = np.squeeze(asArrayGen)\n",
    "            mse = metrics.mean_squared_error(asArrayOrig, squeezed)\n",
    "            #msesPerPoint.append(mse)\n",
    "            totalName = model_names[model_i] + \"_\" + str(feature_quantile).replace(\".\",\"_\") + \"_quantile_MSE\"\n",
    "            data_df.at[data_ix, totalName] = mse\n",
    "            msesPerPoint.append(mse)\n",
    "        msesPerPoint = np.array(msesPerPoint)\n",
    "        feature_quantile_mses.append(msesPerPoint)\n",
    "    model_mses.append(feature_quantile_mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MSEs ---\n",
      "----------------   0.76   0.82   0.88\n",
      "L2X                1.76   1.82   1.89\n",
      "INVASE             1.55   1.70   1.85\n",
      "L2X_FULL_DATA      1.68   1.78   1.91\n",
      "INVASE_FULL_DATA   1.99   2.05   2.09\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Store benchmark results as tables\n",
    "\n",
    "save_figs = False\n",
    "\n",
    "mse_table = np.zeros((len(model_mses), len(model_mses[0])))\n",
    "\n",
    "for i, model_name in enumerate(model_names) :\n",
    "    \n",
    "    for j, feature_quantile in enumerate(feature_quantiles) :\n",
    "        \n",
    "        mse_table[i, j] = np.mean(model_mses[i][j])\n",
    "\n",
    "#Plot and store mse table\n",
    "f = plt.figure(figsize = (4, 6))\n",
    "\n",
    "cells = np.round(mse_table, 3).tolist()\n",
    "\n",
    "print(\"--- MSEs ---\")\n",
    "max_len = np.max([len(model_name.upper().replace(\"\\n\", \" \")) for model_name in model_names])\n",
    "print((\"-\" * max_len) + \"   \" + \"   \".join([(str(feature_quantile) + \"0\")[:4] for feature_quantile in feature_quantiles]))\n",
    "for i in range(len(cells)) :\n",
    "    \n",
    "    curr_len = len([model_name.upper().replace(\"\\n\", \" \") for model_name in model_names][i])\n",
    "    row_str = [model_name.upper().replace(\"\\n\", \" \") for model_name in model_names][i] + (\" \" * (max_len - curr_len))\n",
    "    \n",
    "    for j in range(len(cells[i])) :\n",
    "        cells[i][j] = (str(cells[i][j]) + \"00000\")[:4]\n",
    "        \n",
    "        row_str += \"   \" + cells[i][j]\n",
    "    \n",
    "    print(row_str)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "table = plt.table(cellText=cells, rowLabels=[model_name.upper().replace(\"\\n\", \" \") for model_name in model_names], colLabels=feature_quantiles, loc='center')\n",
    "\n",
    "ax = plt.gca()\n",
    "#f.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if save_figs :\n",
    "    plt.savefig(dataset_name + \"_l2x_and_invase_full_data\" + \"_mse_table.png\", dpi=300, transparent=True)\n",
    "    plt.savefig(dataset_name + \"_l2x_and_invase_full_data\" + \"_mse_table.eps\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
