{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#AML save importance scores IF inclusion optimus scrambler \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import scipy as scp\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import os \n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "#python imports \n",
    "#functions from Optimus 5-Prime model\n",
    "from optimusFunctions import *\n",
    "#dna sequence graphing \n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import scipy as scp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT!  Only run this once due to the registered gradient \n",
    "\n",
    "\n",
    "@ops.RegisterGradient(\"STMul\")\n",
    "def st_mul(op, grad):\n",
    "    return [grad, grad]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def st_sampled_softmax(logits):\n",
    "    with ops.name_scope(\"STSampledSoftmax\") as namescope :\n",
    "        nt_probs = tf.nn.softmax(logits)\n",
    "        onehot_dim = logits.get_shape().as_list()[1]\n",
    "        sampled_onehot = tf.one_hot(tf.squeeze(tf.multinomial(logits, 1), 1), onehot_dim, 1.0, 0.0)\n",
    "        with tf.get_default_graph().gradient_override_map({'Ceil': 'Identity', 'Mul': 'STMul'}):\n",
    "            return tf.ceil(sampled_onehot * nt_probs)\n",
    "\n",
    "def st_hardmax_softmax(logits):\n",
    "    with ops.name_scope(\"STHardmaxSoftmax\") as namescope :\n",
    "        nt_probs = tf.nn.softmax(logits)\n",
    "        onehot_dim = logits.get_shape().as_list()[1]\n",
    "        sampled_onehot = tf.one_hot(tf.argmax(nt_probs, 1), onehot_dim, 1.0, 0.0)\n",
    "        with tf.get_default_graph().gradient_override_map({'Ceil': 'Identity', 'Mul': 'STMul'}):\n",
    "            return tf.ceil(sampled_onehot * nt_probs)\n",
    "        \n",
    "#Gumbel Distribution Sampler\n",
    "def gumbel_softmax(logits, temperature=0.5) :\n",
    "    gumbel_dist = tf.contrib.distributions.RelaxedOneHotCategorical(temperature, logits=logits)\n",
    "    batch_dim = logits.get_shape().as_list()[0]\n",
    "    onehot_dim = logits.get_shape().as_list()[1]\n",
    "    return gumbel_dist.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model functions for loading optimus scramblers \n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def mask_dropout_multi_scale(mask, drop_scales=[1, 2, 4, 7], min_drop_rate=0.0, max_drop_rate=0.5) :\n",
    "    rates = K.random_uniform(shape=(K.shape(mask)[0], 1, 1, 1), minval=min_drop_rate, maxval=max_drop_rate)\n",
    "    scale_logits = K.random_uniform(shape=(K.shape(mask)[0], len(drop_scales), 1, 1, 1), minval=-5., maxval=5.)\n",
    "    scale_probs = K.softmax(scale_logits, axis=1)\n",
    "    ret_mask = mask\n",
    "    for drop_scale_ix, drop_scale in enumerate(drop_scales) :\n",
    "        ret_mask = mask_dropout(ret_mask, rates * scale_probs[:, drop_scale_ix, ...], drop_scale=drop_scale)\n",
    "    return K.switch(K.learning_phase(), ret_mask, mask)\n",
    "def mask_dropout(mask, drop_rates, drop_scale=1) :\n",
    "    random_tensor_downsampled = K.random_uniform(shape=(\n",
    "        K.shape(mask)[0],\n",
    "        1,\n",
    "        K.cast(K.shape(mask)[2] / drop_scale, dtype=tf.int32),\n",
    "        K.shape(mask)[3]\n",
    "    ), minval=0.0, maxval=1.0)\n",
    "    keep_mask_downsampled = random_tensor_downsampled >= drop_rates\n",
    "    keep_mask = K.repeat_elements(keep_mask_downsampled, rep=drop_scale, axis=2)\n",
    "    ret_mask = mask * K.cast(keep_mask, dtype=tf.float32)\n",
    "    return ret_mask\n",
    "def mask_dropout_single_scale(mask, drop_scale=1, min_drop_rate=0.0, max_drop_rate=0.5) :\n",
    "    rates = K.random_uniform(shape=(K.shape(mask)[0], 1, 1, 1), minval=min_drop_rate, maxval=max_drop_rate)\n",
    "    random_tensor_downsampled = K.random_uniform(shape=(\n",
    "        K.shape(mask)[0],\n",
    "        1,\n",
    "        K.cast(K.shape(mask)[2] / drop_scale, dtype=tf.int32),\n",
    "        K.shape(mask)[3]\n",
    "    ), minval=0.0, maxval=1.0)\n",
    "    keep_mask_downsampled = random_tensor_downsampled >= rates\n",
    "    keep_mask = K.repeat_elements(keep_mask_downsampled, rep=drop_scale, axis=2)\n",
    "    ret_mask = mask * K.cast(keep_mask, dtype=tf.float32)\n",
    "    return K.switch(K.learning_phase(), ret_mask, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImportanceScoresForPredictedSet(model_name, predictor_path, x_test, batch_size = 32):\n",
    "    #Load models\n",
    "    save_dir = 'saved_models'\n",
    "\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    scrambler_model = load_model(model_path, custom_objects={\n",
    "        'st_sampled_softmax' : st_sampled_softmax,\n",
    "        'gumbel_softmax': gumbel_softmax\n",
    "        \n",
    "    })\n",
    "\n",
    "    print('Loaded scrambler model %s ' % (model_path))\n",
    "    \n",
    "    #Load Predictor\n",
    "    print (\"loading predictor: \", predictor_path)\n",
    "    predictor = load_model(predictor_path)\n",
    "    predictor.trainable = False\n",
    "    predictor.compile(optimizer=keras.optimizers.SGD(lr=0.1), loss='mean_squared_error')    \n",
    "\n",
    "    #Pad x_test\n",
    "    n_pad = 32 - x_test.shape[0] % 32 if x_test.shape[0] % 32 != 0 else 0\n",
    "    \n",
    "    x_test = np.concatenate([\n",
    "        x_test,\n",
    "        np.zeros((n_pad, x_test.shape[1], x_test.shape[2], x_test.shape[3]))\n",
    "    ], axis=0)\n",
    "\n",
    "    s_test = np.zeros((x_test.shape[0], 1))\n",
    "\n",
    "    _, pwm_test, sample_test, scores = scrambler_model.predict(x=[x_test, s_test])\n",
    "\n",
    "    print (scores.shape)\n",
    "    return scores[:-n_pad]\n",
    "\n",
    "\n",
    "def getImportanceScoresForPredictedSet_dropout(model_name, predictor_path, x_test, batch_size = 32):\n",
    "    \n",
    "    scrambler_model = load_model(model_name, custom_objects={\n",
    "        'st_sampled_softmax' : st_sampled_softmax,\n",
    "        'gumbel_softmax': gumbel_softmax,\n",
    "        \"mask_dropout_multi_scale\": mask_dropout_multi_scale\n",
    "    })\n",
    "    \n",
    "    #Load Predictor\n",
    "    print (\"loading predictor: \", predictor_path)\n",
    "    predictor = load_model(predictor_path)\n",
    "    predictor.trainable = False\n",
    "    predictor.compile(optimizer=keras.optimizers.SGD(lr=0.1), loss='mean_squared_error')    \n",
    "\n",
    "    #Pad x_test\n",
    "    n_pad = 32 - x_test.shape[0] % 32 if x_test.shape[0] % 32 != 0 else 0\n",
    "    \n",
    "    x_test = np.concatenate([\n",
    "        x_test,\n",
    "        np.zeros((n_pad, x_test.shape[1], x_test.shape[2], x_test.shape[3]))\n",
    "    ], axis=0)\n",
    "    \n",
    "    s_test = np.zeros((x_test.shape[0], 1))\n",
    "    drop_pattern = np.ones((x_test.shape[0], 1, 50, 1))\n",
    "\n",
    "    _, pwm_test, sample_test, scores = scrambler_model.predict(x=[x_test, drop_pattern, s_test])\n",
    "\n",
    "    print (scores.shape)\n",
    "    return scores[:-n_pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load Predictor\n",
    "#############################################################\n",
    "predictor_path = 'optimusRetrainedMain.hdf5'\n",
    "model = load_model(predictor_path)\n",
    "\n",
    "sequence_template = \"N\"*50\n",
    "\n",
    "\n",
    "allFiles = [\"optimus5_synthetic_random_insert_if_uorf_1_start_1_stop_variable_loc_512.csv\",\n",
    "            \"optimus5_synthetic_random_insert_if_uorf_1_start_2_stop_variable_loc_512.csv\",\n",
    "            \"optimus5_synthetic_random_insert_if_uorf_2_start_1_stop_variable_loc_512.csv\",\n",
    "            \"optimus5_synthetic_random_insert_if_uorf_2_start_2_stop_variable_loc_512.csv\",\n",
    "            \"optimus5_synthetic_examples_3.csv\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-f43e30867d15>:19: RelaxedOneHotCategorical.__init__ (from tensorflow.contrib.distributions.python.ops.relaxed_onehot_categorical) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/relaxed_onehot_categorical.py:427: ExpRelaxedOneHotCategorical.__init__ (from tensorflow.contrib.distributions.python.ops.relaxed_onehot_categorical) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/relaxed_onehot_categorical.py:214: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/relaxed_onehot_categorical.py:429: Exp.__init__ (from tensorflow.contrib.distributions.python.ops.bijectors.exp) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/bijectors/exp.py:73: PowerTransform.__init__ (from tensorflow.contrib.distributions.python.ops.bijectors.power_transform) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/relaxed_onehot_categorical.py:430: TransformedDistribution.__init__ (from tensorflow.python.ops.distributions.transformed_distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/relaxed_onehot_categorical.py:266: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "loading predictor:  optimusRetrainedMain.hdf5\n",
      "(32, 1, 50, 1)\n",
      "loading predictor:  optimusRetrainedMain.hdf5\n",
      "(32, 1, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "saveDir = \"./\"\n",
    "for csv_to_open in allFiles:\n",
    "    \n",
    "    #Load dataset for benchmarking \n",
    "    dataset_name = csv_to_open.replace(\".csv\", \"\")\n",
    "    benchmarkSet = pd.read_csv(\"./\" + csv_to_open) #open from scores folder \n",
    " \n",
    "\n",
    "    #get correct input shape \n",
    "    seq_e_test = one_hot_encode(benchmarkSet, seq_len=50)\n",
    "    benchmarkSet_seqs = seq_e_test\n",
    "    benchmarkSet_seqs = np.reshape(benchmarkSet_seqs, (benchmarkSet_seqs.shape[0], 1, benchmarkSet_seqs.shape[1], benchmarkSet_seqs.shape[2]))\n",
    "\n",
    "    #m1 \n",
    "    #0.125 model \n",
    "    model_name = \"saved_models/autoscrambler_dataset_egfp_unmod_1_sample_mode_gumbel_n_samples_32_resnet_5_4_32_3_00_00_to_015_n_epochs_50_target_bits_0125_example_if_uorf_seqs_drop_multi_scale_weight_10.h5\"\n",
    "    scores= getImportanceScoresForPredictedSet_dropout(model_name, predictor_path, benchmarkSet_seqs)\n",
    "    save_name = saveDir + \"_10_dropout_autoscrambler_\" + dataset_name\n",
    "    np.save(save_name + \"_importance_scores_test\", scores)\n",
    "    \n",
    "    #m2 \n",
    "    #0.25 model \n",
    "    model_025_IF_only = \"saved_models/autoscrambler_dataset_egfp_unmod_1_sample_mode_gumbel_n_samples_32_resnet_5_4_32_3_00_00_to_015_n_epochs_50_target_bits_0125_example_if_uorf_seqs_drop_multi_scale_weight_1.h5\"\n",
    "    scores  = getImportanceScoresForPredictedSet_dropout(model_025_IF_only, predictor_path, benchmarkSet_seqs)\n",
    "    save_name = saveDir + \"_1_dropout_autoscrambler_\" + dataset_name\n",
    "    np.save(save_name + \"_importance_scores_test\", scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
