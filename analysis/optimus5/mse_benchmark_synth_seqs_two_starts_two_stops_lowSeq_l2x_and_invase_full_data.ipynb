{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#12/29/20\n",
    "#runnign synthetic benchmark graphs for synthetic OR datasets generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#making benchmark images \n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Conv1D, MaxPooling1D, LSTM, ConvLSTM2D, GRU, CuDNNLSTM, CuDNNGRU, BatchNormalization, LocallyConnected2D, Permute, TimeDistributed, Bidirectional\n",
    "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from keras.layers.merge import _Merge\n",
    "import keras.losses\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import isolearn.keras as iso\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import isolearn.io as isoio\n",
    "import isolearn.keras as isol\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as spio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sequence_logo_helper import dna_letter_at, plot_dna_logo\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "def contain_tf_gpu_mem_usage() :\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)\n",
    "\n",
    "contain_tf_gpu_mem_usage()\n",
    "\n",
    "class EpochVariableCallback(Callback) :\n",
    "    \n",
    "    def __init__(self, my_variable, my_func) :\n",
    "        self.my_variable = my_variable       \n",
    "        self.my_func = my_func\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs={}) :\n",
    "        K.set_value(self.my_variable, self.my_func(K.get_value(self.my_variable), epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ONLY RUN THIS CELL ONCE \n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "#Stochastic Binarized Neuron helper functions (Tensorflow)\n",
    "#ST Estimator code adopted from https://r2rt.com/beyond-binary-ternary-and-one-hot-neurons.html\n",
    "#See Github https://github.com/spitis/\n",
    "\n",
    "def st_sampled_softmax(logits):\n",
    "    with ops.name_scope(\"STSampledSoftmax\") as namescope :\n",
    "        nt_probs = tf.nn.softmax(logits)\n",
    "        onehot_dim = logits.get_shape().as_list()[1]\n",
    "        sampled_onehot = tf.one_hot(tf.squeeze(tf.multinomial(logits, 1), 1), onehot_dim, 1.0, 0.0)\n",
    "        with tf.get_default_graph().gradient_override_map({'Ceil': 'Identity', 'Mul': 'STMul'}):\n",
    "            return tf.ceil(sampled_onehot * nt_probs)\n",
    "\n",
    "def st_hardmax_softmax(logits):\n",
    "    with ops.name_scope(\"STHardmaxSoftmax\") as namescope :\n",
    "        nt_probs = tf.nn.softmax(logits)\n",
    "        onehot_dim = logits.get_shape().as_list()[1]\n",
    "        sampled_onehot = tf.one_hot(tf.argmax(nt_probs, 1), onehot_dim, 1.0, 0.0)\n",
    "        with tf.get_default_graph().gradient_override_map({'Ceil': 'Identity', 'Mul': 'STMul'}):\n",
    "            return tf.ceil(sampled_onehot * nt_probs)\n",
    "\n",
    "@ops.RegisterGradient(\"STMul\")\n",
    "def st_mul(op, grad):\n",
    "    return [grad, grad]\n",
    "\n",
    "#Gumbel Distribution Sampler\n",
    "def gumbel_softmax(logits, temperature=0.5) :\n",
    "    gumbel_dist = tf.contrib.distributions.RelaxedOneHotCategorical(temperature, logits=logits)\n",
    "    batch_dim = logits.get_shape().as_list()[0]\n",
    "    onehot_dim = logits.get_shape().as_list()[1]\n",
    "    return gumbel_dist.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model functions for loading optimus scramblers \n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def mask_dropout_multi_scale(mask, drop_scales=[1, 2, 4, 7], min_drop_rate=0.0, max_drop_rate=0.5) :\n",
    "    rates = K.random_uniform(shape=(K.shape(mask)[0], 1, 1, 1), minval=min_drop_rate, maxval=max_drop_rate)\n",
    "    scale_logits = K.random_uniform(shape=(K.shape(mask)[0], len(drop_scales), 1, 1, 1), minval=-5., maxval=5.)\n",
    "    scale_probs = K.softmax(scale_logits, axis=1)\n",
    "    ret_mask = mask\n",
    "    for drop_scale_ix, drop_scale in enumerate(drop_scales) :\n",
    "        ret_mask = mask_dropout(ret_mask, rates * scale_probs[:, drop_scale_ix, ...], drop_scale=drop_scale)\n",
    "    return K.switch(K.learning_phase(), ret_mask, mask)\n",
    "def mask_dropout(mask, drop_rates, drop_scale=1) :\n",
    "    random_tensor_downsampled = K.random_uniform(shape=(\n",
    "        K.shape(mask)[0],\n",
    "        1,\n",
    "        K.cast(K.shape(mask)[2] / drop_scale, dtype=tf.int32),\n",
    "        K.shape(mask)[3]\n",
    "    ), minval=0.0, maxval=1.0)\n",
    "    keep_mask_downsampled = random_tensor_downsampled >= drop_rates\n",
    "    keep_mask = K.repeat_elements(keep_mask_downsampled, rep=drop_scale, axis=2)\n",
    "    ret_mask = mask * K.cast(keep_mask, dtype=tf.float32)\n",
    "    return ret_mask\n",
    "def mask_dropout_single_scale(mask, drop_scale=1, min_drop_rate=0.0, max_drop_rate=0.5) :\n",
    "    rates = K.random_uniform(shape=(K.shape(mask)[0], 1, 1, 1), minval=min_drop_rate, maxval=max_drop_rate)\n",
    "    random_tensor_downsampled = K.random_uniform(shape=(\n",
    "        K.shape(mask)[0],\n",
    "        1,\n",
    "        K.cast(K.shape(mask)[2] / drop_scale, dtype=tf.int32),\n",
    "        K.shape(mask)[3]\n",
    "    ), minval=0.0, maxval=1.0)\n",
    "    keep_mask_downsampled = random_tensor_downsampled >= rates\n",
    "    keep_mask = K.repeat_elements(keep_mask_downsampled, rep=drop_scale, axis=2)\n",
    "    ret_mask = mask * K.cast(keep_mask, dtype=tf.float32)\n",
    "    return K.switch(K.learning_phase(), ret_mask, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PWM Masking and Sampling helper functions\n",
    "\n",
    "def mask_pwm(inputs) :\n",
    "    pwm, onehot_template, onehot_mask = inputs\n",
    "\n",
    "    return pwm * onehot_mask + onehot_template\n",
    "\n",
    "def sample_pwm_st(pwm_logits) :\n",
    "    n_sequences = K.shape(pwm_logits)[0]\n",
    "    seq_length = K.shape(pwm_logits)[2]\n",
    "\n",
    "    flat_pwm = K.reshape(pwm_logits, (n_sequences * seq_length, 4))\n",
    "    sampled_pwm = st_sampled_softmax(flat_pwm)\n",
    "\n",
    "    return K.reshape(sampled_pwm, (n_sequences, 1, seq_length, 4))\n",
    "\n",
    "def sample_pwm_gumbel(pwm_logits) :\n",
    "    n_sequences = K.shape(pwm_logits)[0]\n",
    "    seq_length = K.shape(pwm_logits)[2]\n",
    "\n",
    "    flat_pwm = K.reshape(pwm_logits, (n_sequences * seq_length, 4))\n",
    "    sampled_pwm = gumbel_softmax(flat_pwm, temperature=0.5)\n",
    "\n",
    "    return K.reshape(sampled_pwm, (n_sequences, 1, seq_length, 4))\n",
    "\n",
    "#Generator helper functions\n",
    "def initialize_sequence_templates(generator, sequence_templates, background_matrices) :\n",
    "\n",
    "    embedding_templates = []\n",
    "    embedding_masks = []\n",
    "    embedding_backgrounds = []\n",
    "\n",
    "    for k in range(len(sequence_templates)) :\n",
    "        sequence_template = sequence_templates[k]\n",
    "        onehot_template = iso.OneHotEncoder(seq_length=len(sequence_template))(sequence_template).reshape((1, len(sequence_template), 4))\n",
    "\n",
    "        for j in range(len(sequence_template)) :\n",
    "            if sequence_template[j] not in ['N', 'X'] :\n",
    "                nt_ix = np.argmax(onehot_template[0, j, :])\n",
    "                onehot_template[:, j, :] = -4.0\n",
    "                onehot_template[:, j, nt_ix] = 10.0\n",
    "            elif sequence_template[j] == 'X' :\n",
    "                onehot_template[:, j, :] = -1.0\n",
    "\n",
    "        onehot_mask = np.zeros((1, len(sequence_template), 4))\n",
    "        for j in range(len(sequence_template)) :\n",
    "            if sequence_template[j] == 'N' :\n",
    "                onehot_mask[:, j, :] = 1.0\n",
    "\n",
    "        embedding_templates.append(onehot_template.reshape(1, -1))\n",
    "        embedding_masks.append(onehot_mask.reshape(1, -1))\n",
    "        embedding_backgrounds.append(background_matrices[k].reshape(1, -1))\n",
    "\n",
    "    embedding_templates = np.concatenate(embedding_templates, axis=0)\n",
    "    embedding_masks = np.concatenate(embedding_masks, axis=0)\n",
    "    embedding_backgrounds = np.concatenate(embedding_backgrounds, axis=0)\n",
    "\n",
    "    generator.get_layer('template_dense').set_weights([embedding_templates])\n",
    "    generator.get_layer('template_dense').trainable = False\n",
    "\n",
    "    generator.get_layer('mask_dense').set_weights([embedding_masks])\n",
    "    generator.get_layer('mask_dense').trainable = False\n",
    "    \n",
    "    generator.get_layer('background_dense').set_weights([embedding_backgrounds])\n",
    "    generator.get_layer('background_dense').trainable = False\n",
    "\n",
    "#Generator construction function\n",
    "def build_sampler(batch_size, seq_length, n_classes=1, n_samples=1, sample_mode='st') :\n",
    "\n",
    "    #Initialize Reshape layer\n",
    "    reshape_layer = Reshape((1, seq_length, 4))\n",
    "    \n",
    "    #Initialize background matrix\n",
    "    onehot_background_dense = Embedding(n_classes, seq_length * 4, embeddings_initializer='zeros', name='background_dense')\n",
    "\n",
    "    #Initialize template and mask matrices\n",
    "    onehot_template_dense = Embedding(n_classes, seq_length * 4, embeddings_initializer='zeros', name='template_dense')\n",
    "    onehot_mask_dense = Embedding(n_classes, seq_length * 4, embeddings_initializer='ones', name='mask_dense')\n",
    "\n",
    "    #Initialize Templating and Masking Lambda layer\n",
    "    masking_layer = Lambda(mask_pwm, output_shape = (1, seq_length, 4), name='masking_layer')\n",
    "    background_layer = Lambda(lambda x: x[0] + x[1], name='background_layer')\n",
    "    \n",
    "    #Initialize PWM normalization layer\n",
    "    pwm_layer = Softmax(axis=-1, name='pwm')\n",
    "    \n",
    "    #Initialize sampling layers\n",
    "    sample_func = None\n",
    "    if sample_mode == 'st' :\n",
    "        sample_func = sample_pwm_st\n",
    "    elif sample_mode == 'gumbel' :\n",
    "        sample_func = sample_pwm_gumbel\n",
    "    \n",
    "    upsampling_layer = Lambda(lambda x: K.tile(x, [n_samples, 1, 1, 1]), name='upsampling_layer')\n",
    "    sampling_layer = Lambda(sample_func, name='pwm_sampler')\n",
    "    permute_layer = Lambda(lambda x: K.permute_dimensions(K.reshape(x, (n_samples, batch_size, 1, seq_length, 4)), (1, 0, 2, 3, 4)), name='permute_layer')\n",
    "    \n",
    "    def _sampler_func(class_input, raw_logits) :\n",
    "        \n",
    "        #Get Template and Mask\n",
    "        onehot_background = reshape_layer(onehot_background_dense(class_input))\n",
    "        onehot_template = reshape_layer(onehot_template_dense(class_input))\n",
    "        onehot_mask = reshape_layer(onehot_mask_dense(class_input))\n",
    "        \n",
    "        #Add Template and Multiply Mask\n",
    "        pwm_logits = masking_layer([background_layer([raw_logits, onehot_background]), onehot_template, onehot_mask])\n",
    "        \n",
    "        #Compute PWM (Nucleotide-wise Softmax)\n",
    "        pwm = pwm_layer(pwm_logits)\n",
    "        \n",
    "        #Tile each PWM to sample from and create sample axis\n",
    "        pwm_logits_upsampled = upsampling_layer(pwm_logits)\n",
    "        sampled_pwm = sampling_layer(pwm_logits_upsampled)\n",
    "        sampled_pwm = permute_layer(sampled_pwm)\n",
    "\n",
    "        sampled_mask = permute_layer(upsampling_layer(onehot_mask))\n",
    "        \n",
    "        return pwm_logits, pwm, sampled_pwm, onehot_mask, sampled_mask\n",
    "    \n",
    "    return _sampler_func\n",
    "\n",
    "#for formulation 2 graphing \n",
    "def returnXMeanLogits(e_train):\n",
    "    #returns x mean logits for displayign the pwm difference for the version 2 networks \n",
    "    #Visualize background sequence distribution\n",
    "    seq_e_train = one_hot_encode(e_train,seq_len=50)\n",
    "    x_train = seq_e_train\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1], x_train.shape[2]))\n",
    "\n",
    "    pseudo_count = 1.0\n",
    "\n",
    "    x_mean = (np.sum(x_train, axis=(0, 1)) + pseudo_count) / (x_train.shape[0] + 4. * pseudo_count)\n",
    "    x_mean_logits = np.log(x_mean / (1. - x_mean))\n",
    "    return x_mean_logits, x_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimus5_synthetic_random_insert_if_uorf_2_start_2_stop_variable_loc_512\n",
      "(512, 1, 50, 4)\n"
     ]
    }
   ],
   "source": [
    "#loading testing dataset \n",
    "\n",
    "from optimusFunctions import *\n",
    "import pandas as pd\n",
    "\n",
    "csv_to_open = \"optimus5_synthetic_random_insert_if_uorf_2_start_2_stop_variable_loc_512.csv\"\n",
    "\n",
    "\n",
    "dataset_name = csv_to_open.replace(\".csv\", \"\")\n",
    "print (dataset_name)\n",
    "data_df = pd.read_csv(\"./\" + csv_to_open) #open from scores folder \n",
    "#loaded test set which is sorted by number of start/stop signals \n",
    "\n",
    "seq_e_test = one_hot_encode(data_df, seq_len=50)\n",
    "benchmarkSet_seqs = seq_e_test\n",
    "x_test = np.reshape(benchmarkSet_seqs, (benchmarkSet_seqs.shape[0], 1, benchmarkSet_seqs.shape[1], benchmarkSet_seqs.shape[2]))\n",
    "print (x_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training:  15008  testing:  512\n"
     ]
    }
   ],
   "source": [
    "e_train = pd.read_csv(\"bottom5KIFuAUGTop5KIFuAUG.csv\")\n",
    "print (\"training: \", e_train.shape[0], \" testing: \", x_test.shape[0])\n",
    "\n",
    "#one hot encode with optimus encoders \n",
    "seq_e_train = one_hot_encode(e_train,seq_len=50)\n",
    "x_mean_logits, x_mean = returnXMeanLogits(e_train)\n",
    "seq_e_train = one_hot_encode(e_train,seq_len=50)\n",
    "x_train = seq_e_train\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1], x_train.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training:  15008  testing:  512\n"
     ]
    }
   ],
   "source": [
    "#background \n",
    "\n",
    "\n",
    "#for formulation 2 graphing \n",
    "def returnXMeanLogits(e_train):\n",
    "    #returns x mean logits for displayign the pwm difference for the version 2 networks \n",
    "    #Visualize background sequence distribution\n",
    "    seq_e_train = one_hot_encode(e_train,seq_len=50)\n",
    "    x_train = seq_e_train\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1], x_train.shape[2]))\n",
    "\n",
    "    pseudo_count = 1.0\n",
    "\n",
    "    x_mean = (np.sum(x_train, axis=(0, 1)) + pseudo_count) / (x_train.shape[0] + 4. * pseudo_count)\n",
    "    x_mean_logits = np.log(x_mean / (1. - x_mean))\n",
    "    return x_mean_logits, x_mean\n",
    "\n",
    "\n",
    "e_train = pd.read_csv(\"bottom5KIFuAUGTop5KIFuAUG.csv\")\n",
    "print (\"training: \", e_train.shape[0], \" testing: \", x_test.shape[0])\n",
    "#one hot encode with optimus encoders \n",
    "seq_e_train = one_hot_encode(e_train,seq_len=50)\n",
    "x_mean_logits, x_mean = returnXMeanLogits(e_train)\n",
    "x_train = seq_e_train\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1], x_train.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+gAAAAnCAYAAACc9WIYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAAXhJREFUeJzt3LFKQ0EURdEzYm2plY3xQ/z/ysLCLj9gLTZyLZJYiGCCSA6yFrxq7jDTbgbempkAAAAA53Vx7gsAAAAAAh0AAAAqCHQAAAAoINABAACggEAHAACAAgIdAAAACgh0AAAAKCDQAQAAoIBABwAAgAICHQAAAAoIdAAAACgg0AEAAKCAQAcAAIACAh0AAAAKCHQAAAAoINABAACggEAHAACAAgIdAAAACgh0AAAAKCDQAQAAoIBABwAAgAICHQAAAAoIdAAAACgg0AEAAKDA5akb1somyUOSuyQ3Sa6SvCV5SvKS5DXJe5LZf59b9+c9z+TxV7cGAACAf+bkQE9ym+Q+ySbJdXaBvs0uwNeX2UOgf7cGAAAA7K2Z+XnqMLzW8cMAAABAZuaoB+uTAh0AAAD4G34SBwAAAAUEOgAAABQQ6AAAAFBAoAMAAEABgQ4AAAAFBDoAAAAUEOgAAABQQKADAABAAYEOAAAABQQ6AAAAFPgAbx8asfW9BQIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1008x46.8 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean conservation (bits) = 0.032049298346210106\n",
      "Mean KL Div against background (bits) = 1.9679329305814974\n"
     ]
    }
   ],
   "source": [
    "#Define sequence template for optimus\n",
    "\n",
    "sequence_template = 'N'*50\n",
    "sequence_mask = np.array([1 if sequence_template[j] == 'N' else 0 for j in range(len(sequence_template))])\n",
    "\n",
    "#Visualize background sequence distribution\n",
    "\n",
    "save_figs = True\n",
    "plot_dna_logo(np.copy(x_mean), sequence_template=sequence_template, figsize=(14, 0.65), logo_height=1.0, plot_start=0, plot_end=50)\n",
    "\n",
    "#Calculate mean training set conservation\n",
    "\n",
    "entropy = np.sum(x_mean * -np.log(x_mean), axis=-1) / np.log(2.0)\n",
    "conservation = 2.0 - entropy\n",
    "x_mean_conservation = np.sum(conservation) / np.sum(sequence_mask)\n",
    "print(\"Mean conservation (bits) = \" + str(x_mean_conservation))\n",
    "\n",
    "#Calculate mean training set kl-divergence against background\n",
    "x_train_clipped = np.clip(np.copy(x_train[:, 0, :, :]), 1e-8, 1. - 1e-8)\n",
    "kl_divs = np.sum(x_train_clipped * np.log(x_train_clipped / np.tile(np.expand_dims(x_mean, axis=0), (x_train_clipped.shape[0], 1, 1))), axis=-1) / np.log(2.0)\n",
    "x_mean_kl_divs = np.sum(kl_divs * sequence_mask, axis=-1) / np.sum(sequence_mask)\n",
    "x_mean_kl_div = np.mean(x_mean_kl_divs)\n",
    "print(\"Mean KL Div against background (bits) = \" + str(x_mean_kl_div))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initialize Encoder and Decoder networks\n",
    "batch_size = 32\n",
    "seq_length = 50\n",
    "n_samples = 128\n",
    "sample_mode = 'st'\n",
    "#sample_mode = 'gumbel'\n",
    "\n",
    "#Load sampler\n",
    "sampler = build_sampler(batch_size, seq_length, n_classes=1, n_samples=n_samples, sample_mode=sample_mode)\n",
    "\n",
    "#Load Predictor\n",
    "predictor_path = 'optimusRetrainedMain.hdf5'\n",
    "predictor = load_model(predictor_path)\n",
    "predictor.trainable = False\n",
    "predictor.compile(optimizer=keras.optimizers.SGD(lr=0.1), loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Build scrambler model\n",
    "dummy_class = Input(shape=(1,), name='dummy_class')\n",
    "input_logits = Input(shape=(1, seq_length, 4), name='input_logits')\n",
    "\n",
    "pwm_logits, pwm, sampled_pwm, pwm_mask, sampled_mask = sampler(dummy_class, input_logits)\n",
    "\n",
    "scrambler_model = Model([input_logits, dummy_class], [pwm_logits, pwm, sampled_pwm, pwm_mask, sampled_mask])\n",
    "\n",
    "#Initialize Sequence Templates and Masks\n",
    "initialize_sequence_templates(scrambler_model, [sequence_template], [x_mean_logits])\n",
    "\n",
    "scrambler_model.trainable = False\n",
    "scrambler_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999),\n",
    "    loss='mean_squared_error'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "#open all score and reshape as needed \n",
    "\n",
    "file_names = [\n",
    "    \"l2x_\" + dataset_name +  \"_importance_scores_test.npy\",\n",
    "    \"invase_\" + dataset_name +  \"_conv_importance_scores_test.npy\",\n",
    "    \"l2x_\" + dataset_name +  \"_full_data_importance_scores_test.npy\",\n",
    "    \"invase_\" + dataset_name +  \"_conv_full_data_importance_scores_test.npy\",\n",
    "]\n",
    "#deepexplain_optimus_utr_OR_logic_synth_1_start_2_stops_method_integrated_gradients_importance_scores_test.npy\n",
    "\n",
    "model_names =[\n",
    "    \"l2x\",\n",
    "    \"invase\",\n",
    "    \"l2x_full_data\",\n",
    "    \"invase_full_data\",\n",
    "]\n",
    "\n",
    "model_importance_scores_test = [np.load(\"./\" + file_name) for file_name in file_names]\n",
    "\n",
    "for scores in model_importance_scores_test:\n",
    "    print (scores.shape)\n",
    "\n",
    "for model_i in range(len(model_names)) :\n",
    "    if model_importance_scores_test[model_i].shape[-1] > 1 :\n",
    "        model_importance_scores_test[model_i] = np.sum(model_importance_scores_test[model_i], axis=-1, keepdims=True)\n",
    "\n",
    "for scores in model_importance_scores_test:\n",
    "    print (scores.shape)\n",
    "    \n",
    "#reshape for mse script -> if not (3008, 1, 50, 1) make it that shape \n",
    "idealShape = model_importance_scores_test[0].shape\n",
    "print (idealShape)\n",
    "\n",
    "for model_i in range(len(model_names)) :\n",
    "    if model_importance_scores_test[model_i].shape != idealShape:\n",
    "        model_importance_scores_test[model_i] = np.expand_dims(model_importance_scores_test[model_i], 1)\n",
    "        \n",
    "for scores in model_importance_scores_test:\n",
    "    print (scores.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 1, 50, 4)\n",
      "(512, 1, 50, 4)\n",
      "(512, 1)\n",
      "512/512 [==============================] - 2s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "on_state_logit_val = 50.\n",
    "print (x_test.shape)\n",
    "\n",
    "dummy_test = np.zeros((x_test.shape[0], 1))\n",
    "x_test_logits = 2. * x_test - 1.\n",
    "\n",
    "print (x_test_logits.shape)\n",
    "print (dummy_test.shape)\n",
    "\n",
    "\n",
    "x_test_squeezed = np.squeeze(x_test)\n",
    "y_pred_ref = predictor.predict([x_test_squeezed], batch_size=32, verbose=True)[0]\n",
    "\n",
    "_, _, _, pwm_mask, sampled_mask = scrambler_model.predict([x_test_logits, dummy_test], batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'utr', 'gt', 'orig', 'l2x_0_76_quantile_MSE',\n",
      "       'l2x_0_82_quantile_MSE', 'l2x_0_88_quantile_MSE',\n",
      "       'invase_0_76_quantile_MSE', 'invase_0_82_quantile_MSE',\n",
      "       'invase_0_88_quantile_MSE', 'l2x_full_data_0_76_quantile_MSE',\n",
      "       'l2x_full_data_0_82_quantile_MSE', 'l2x_full_data_0_88_quantile_MSE',\n",
      "       'invase_full_data_0_76_quantile_MSE',\n",
      "       'invase_full_data_0_82_quantile_MSE',\n",
      "       'invase_full_data_0_88_quantile_MSE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "feature_quantiles = [0.76, 0.82, 0.88]\n",
    "\n",
    "for name in model_names:\n",
    "    for quantile in feature_quantiles:\n",
    "        totalName = name + \"_\" + str(quantile).replace(\".\",\"_\") + \"_quantile_MSE\"\n",
    "        data_df[totalName] = None\n",
    "    \n",
    "print (data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking model 'l2x'...\n",
      "Feature quantile = 0.76\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.82\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.88\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Benchmarking model 'invase'...\n",
      "Feature quantile = 0.76\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.82\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.88\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Benchmarking model 'l2x_full_data'...\n",
      "Feature quantile = 0.76\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.82\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.88\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Benchmarking model 'invase_full_data'...\n",
      "Feature quantile = 0.76\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.82\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.88\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_quantiles = [0.76, 0.82, 0.88]\n",
    "\n",
    "#batch_size = 128 \n",
    "from sklearn import metrics\n",
    "model_mses = []\n",
    "for model_i in range(len(model_names)) :\n",
    "    \n",
    "    print(\"Benchmarking model '\" + str(model_names[model_i]) + \"'...\")\n",
    "    \n",
    "    feature_quantile_mses = []\n",
    "    \n",
    "    for feature_quantile_i, feature_quantile in enumerate(feature_quantiles) :\n",
    "        \n",
    "        print(\"Feature quantile = \" + str(feature_quantile))\n",
    "    \n",
    "        if len(model_importance_scores_test[model_i].shape) >= 5 :\n",
    "            importance_scores_test = np.abs(model_importance_scores_test[model_i][feature_quantile_i, ...])\n",
    "        else :\n",
    "            importance_scores_test = np.abs(model_importance_scores_test[model_i])\n",
    "        \n",
    "        n_to_test = importance_scores_test.shape[0] // batch_size * batch_size\n",
    "        importance_scores_test = importance_scores_test[:n_to_test]\n",
    "        \n",
    "        importance_scores_test *= np.expand_dims(np.max(pwm_mask[:n_to_test], axis=-1), axis=-1)\n",
    "\n",
    "        quantile_vals = np.quantile(importance_scores_test, axis=(1, 2, 3), q=feature_quantile, keepdims=True)\n",
    "        quantile_vals = np.tile(quantile_vals, (1, importance_scores_test.shape[1], importance_scores_test.shape[2], importance_scores_test.shape[3]))\n",
    "\n",
    "        top_logits_test = np.zeros(importance_scores_test.shape)\n",
    "        top_logits_test[importance_scores_test > quantile_vals] = on_state_logit_val\n",
    "        \n",
    "        top_logits_test = np.tile(top_logits_test, (1, 1, 1, 4)) * x_test_logits[:n_to_test]\n",
    "\n",
    "        _, _, samples_test, _, _ = scrambler_model.predict([top_logits_test, dummy_test[:n_to_test]], batch_size=batch_size)\n",
    "        print (samples_test.shape)\n",
    "        msesPerPoint = []\n",
    "        for data_ix in range(samples_test.shape[0]) :\n",
    "            #for each sample, look at kl divergence for the 128 size batch generated \n",
    "            #for MSE, just track the pred vs original pred \n",
    "            if data_ix % 1000 == 0 :\n",
    "                print(\"Processing example \" + str(data_ix) + \"...\")\n",
    "            \n",
    "            #from optimus R^2, MSE, Pearson R script \n",
    "            justPred = np.expand_dims(np.expand_dims(x_test[data_ix, 0, :, :], axis=0), axis=-1)\n",
    "            justPredReshape = np.reshape(justPred, (1,50,4))\n",
    "            \n",
    "            expanded = np.expand_dims(samples_test[data_ix, :, 0, :, :], axis=-1) #batch size is 128 \n",
    "            expandedReshape = np.reshape(expanded, (n_samples, 50,4))\n",
    "            \n",
    "            y_test_hat_ref = predictor.predict(x=justPredReshape, batch_size=1)[0][0]\n",
    "            \n",
    "            y_test_hat = predictor.predict(x=[expandedReshape], batch_size=32)\n",
    "            \n",
    "            pwmGenerated = y_test_hat.tolist()\n",
    "            tempOriginals = [y_test_hat_ref]*y_test_hat.shape[0]\n",
    "            \n",
    "            asArrayOrig = np.array(tempOriginals)\n",
    "            asArrayGen = np.array(pwmGenerated)\n",
    "            squeezed = np.squeeze(asArrayGen)\n",
    "            mse = metrics.mean_squared_error(asArrayOrig, squeezed)\n",
    "            #msesPerPoint.append(mse)\n",
    "            totalName = model_names[model_i] + \"_\" + str(feature_quantile).replace(\".\",\"_\") + \"_quantile_MSE\"\n",
    "            data_df.at[data_ix, totalName] = mse\n",
    "            msesPerPoint.append(mse)\n",
    "        msesPerPoint = np.array(msesPerPoint)\n",
    "        feature_quantile_mses.append(msesPerPoint)\n",
    "    model_mses.append(feature_quantile_mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MSEs ---\n",
      "----------------   0.76   0.82   0.88\n",
      "L2X                2.82   2.86   2.94\n",
      "INVASE             1.83   2.20   2.67\n",
      "L2X_FULL_DATA      2.66   2.77   2.94\n",
      "INVASE_FULL_DATA   3.16   3.23   3.26\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWkAAAGoCAYAAACEzxRfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xlw1fW9//FXGsQgShEhaAhLDhCy5wSGAG4YIuotQmUphLG5SLWjFmesFi7LnVuZsVhF4Fat2uqlAYsE0GrZFFzAKy4VESJUUBASloQCrQl7AiHv+0d+fH8cs51AcvIhPB8zmeF8l/P9nPc5eXHyXd7fMDMTAMBNP2jqAQAAakZIA4DDCGkAcBghDQAOI6QBwGGENAA4jJAGAIcR0gDgMEIaABxGSAOAwwhpAHAYIQ0ADiOkAcBhhDQAOIyQBgCHEdIA4DBCGgAcRkgDgMMIaQBwGCENAA4jpAHAYYQ0ADiMkAYAhxHSAOAwQhoAHEZIA4DDCGkAcBghDQAOI6QBwGGENAA4jJAGAIcR0gDgMEIaABxGSAOAwwhpAHAYIQ0ADiOkAcBhhDQAOIyQBgCHEdIA4DBCGgAcRkgDgMMIaQBwGCENAA4jpAHAYYQ0ADiMkAYAhxHSAOAwQhoAHEZIA4DDCGkAcBghDQAOI6QBwGGENAA4jJAGAIcR0gDgMEIaABxGSAOAwwhpAHAYIQ0ADiOkAcBhhDQAOIyQBgCHEdIA4DBCGgAcRkgDgMMIaQBwGCENAA4jpAHAYYQ0ADiMkAYAhxHSAOAwQhoAHEZIA4DDCGkAcBghDQAOI6QBwGGENAA4jJAGAIcR0gDgMEIaABxGSAOAwwhpAHAYIQ0ADiOkAcBhhDQAOIyQBgCHEdIA4DBCGgAcRkgDgMMIaQBwGCENAA4jpAHAYYQ0ADiMkAYAhxHSAOAwQhoAHEZIA4DDCGkAcBghDQAOI6QBwGGENAA4jJAGAIcR0gDgMEIaABxGSAOAwwhpAHAYIQ0ADiOkAcBhhDQAOIyQBgCHEdIA4DBCGgAcRkgDgMMIaQBwGCENAA4jpAHAYYQ0ADiMkAYAhxHSAOAwQhoAHEZIA4DDCGkAcBghDQAOI6QBwGGENAA4jJAGAIcR0gDgMEIaABxGSAOAwwhpAHAYIQ0ADiOkAcBhhDQAOIyQBgCHEdIA4DBCGgAcRkgDgMMIaQBwGCENAA4jpAHAYYQ0ADiMkAYAhxHSAOAwQhoAHEZIA4DDCGkAcBghDQAOI6QBwGGENAA4jJAGAIcR0gDgMEIaABxGSAOAwwhpAHAYIQ0ADiOkAcBhhDQAOIyQBgCHEdIA4DBCGgAcRkgDgMMIaQBwGCENAA4jpAHAYYQ0ADiMkAYAhxHSAOCwFk09AIReq1at/lFaWtqxqcdxMYiIiKgoLS3ly0wQqFX9REREHDh58uS1dS0XZmahGA8cEhYWZrzvwQkLCxO1Cg61qp//V6+wupbjfz0AcBghjUa3atUq9erVSz169NCTTz5ZZf4jjzwiv98vv9+v2NhYtW3b1pu3Z88e3XbbbYqPj1dCQoIKCgpCOPLQq6tWe/bsUUZGhtLS0pSSkqK33npLkvTuu++qT58+Sk5OVp8+fbRmzZpQD71JnG+9Tp8+rXHjxik5OVnx8fH67W9/G+qhB8/M+LnEfirf9tAoLy83n89nO3futLKyMktJSbGvvvqqxuWfffZZGz9+vPd44MCB9s4775iZ2dGjR+348eONPuZzuVarn//85/bCCy+YmdlXX31lXbt2NTOzjRs3WmFhoZmZbdmyxaKiokI27rNCWSuzC6vXq6++amPGjDEzs+PHj1vXrl0tPz8/lMM/W686f1/5Jo1GtX79evXo0UM+n08tW7ZUVlaWli5dWuPyubm5Gjt2rCRp69atKi8v1+DBgyVJV155pa644oqQjLspBFOrsLAwHTlyRJJ0+PBhRUVFSZLS0tK8fycmJurkyZMqKysL7QsIsQupV1hYmI4fP67y8nKdPHlSLVu2VJs2bUL+GoJBSKNRFRYWqnPnzt7j6OhoFRYWVrvs7t27lZ+fr0GDBkmStm/frrZt22rEiBFKS0vTpEmTdObMmZCMuykEU6vp06drwYIFio6O1o9+9CM999xzVZ7nL3/5i3r37q3LL7+80cfclC6kXqNGjVLr1q113XXXqUuXLpo4caLatWsX0vEHi5CGMxYtWqRRo0YpPDxcklReXq5169Zp1qxZ+vzzz7Vr1y7NmzevaQfZxHJzc3XPPfdo3759euutt5Sdna2Kigpv/ldffaXJkyfrj3/8YxOO0h011Wv9+vUKDw9XUVGR8vPzNXv2bO3atauph1stQhqNqlOnTtq7d6/3eN++ferUqVO1yy5atMjb1SFVfjPy+/3y+Xxq0aKF7rrrLm3cuLHRx9xUgqnV3LlzNXr0aEnSgAEDVFpaqn/+85/e8sOHD9crr7yi7t27h27gTeRC6rVw4ULdcccduuyyyxQZGakbbrhBGzZsCOn4g0VIo1H17dtXO3bsUH5+vk6dOqVFixZp2LBhVZb7+uuvVVxcrAEDBgSsW1JSokOHDkmS1qxZo4SEhJCNPdSCqVWXLl30/vvvS5K2bdum0tJSdejQQSUlJRoyZIiefPJJ3XDDDU0x/JC7kHp16dLFOwPm+PHj+tvf/qa4uLiQv4agBHN0kZ/m9aMQH4VfuXKl9ezZ03w+n/3mN78xM7P/+q//sqVLl3rLPPbYYzZ58uQq677zzjuWnJxsSUlJNm7cOCsrKwvZuM0s5Gcs1FWrr776yq6//npLSUmx1NRUW716tZmZPf7443bFFVdYamqq93PgwIGQjj3UtTI7/3odPXrURo0aZQkJCRYfH28zZ84M+dgV5NkdXHF4CeKKw+BxFV3wqFX9cMUhADQDhDQAOKzWLnh0S2ueIiIiFBZW519ZELWqD2pVPxERERV1L1VHFzz2XTZP7DsMHrUKHrWqH/ZJA0Az0OAhfeWVV1aZNmfOHCUkJCglJUWZmZnavXu3JGnDhg1KTEzUqVOnJEk7d+6Uz+fzrrUHzrV3715lZGQoISFBiYmJeuaZZ6osc/jwYQ0dOlSpqalKTExUTk6OJCkvL08DBgxQYmKiUlJStHjx4lAPP6QupFbSpdV9MJhaFRcXa/jw4UpJSVF6err+/ve/B8w/c+aM0tLSdOeddzb8AGs7P0/ncd5j69atq0xbs2aN173shRdesNGjR3vzHnzwQZsxY4aZmd1+++22cOHCem8T9XM+76sLioqK7IsvvjAzsyNHjljPnj2rdD2bMWOG/cd//IeZmR08eNCuvvpqKysrs2+++ca2b99uZmaFhYV27bXXWnFxcZ3bvBRrZXZ+3Qebc60mTpxo06dPNzOzbdu22aBBgwLmz54928aOHWtDhgwJertyqQteRkaG172sf//+2rdvnzfviSee0Msvv6yZM2eqvLw84LJg4FzXXXedevfuLUm66qqrFB8fX6WhTlhYmI4ePSoz07Fjx9SuXTu1aNFCsbGx6tmzpyQpKipKkZGR3pWMzdGF1OpS6z4YTK22bt3qNf6Ki4tTQUGBDhw4IKnycvSVK1fqvvvua5TxhXyf9Ny5c/Vv//Zv3uO2bdtqypQpmjp1qp5//vlQDwcXqYKCAm3atEn9+vULmP7QQw9p27ZtioqKUnJysp555hn94AeBH/P169fr1KlTl0R/C6n+tbrUug+eq6Zapaam6o033pBU+fnZvXu392Xzl7/8pWbOnFnlc9ZQQhrSCxYs0IYNGzRp0qSA6W+//bY6duyorVu3hnI4uEgdO3ZMI0eO1O9+97sqPYBXr14tv9+voqIi5eXl6aGHHgo4xrF//35lZ2crJyen0X6pXHI+tbpUuw/WVqspU6aopKREfr9fzz33nNLS0hQeHq4VK1YoMjJSffr0abyB1bYvRA20T9rM7N1337W4uLgq/QSWL19ut9xyi23evNm6d+8e8jtvXIrO5311xalTp+y2226z2bNnVzv/Rz/6kX344Yfe44yMDPvss8/MzOzw4cOWlpZmr732WtDbuxRr9emnn9rNN9/sTX/llVfsF7/4RZ3ba861OldFRYV17drVDh8+bFOmTLFOnTpZ165drWPHjtaqVSu7++67g9qmgtwnHZKQ3rhxo/l8Pu/AzVknTpwI2En/6KOP2rRp0+q9TdTPxfrLVFFRYdnZ2fbwww/XuMwDDzxgjz32mJmZ/eMf/7CoqCg7dOiQlZWV2aBBg+y///u/67XNS7FW5eXllpKSYgcPHjQzs3vuucd+//vf17nN5lyr4uJi76DqSy+9ZNnZ2VWWWbt2baMcOGzwi1l+8IMfeLeokaRHH31Ub731lrZs2aLrrrtOUmX7wGXLlmnatGk6c+aMnnrqKUnS0aNHlZqaqtWrV3sHedDwLtaLDj766CPddNNNSk5O9nZVPPHEE9qzZ48k6YEHHlBRUZHuuece7d+/X2amKVOm6Kc//akWLFig8ePHKzEx0Xu+efPmye/317rNS7FWUuWNbX/1q1/JzNSnTx+99NJLatmyZa3bbM61+vTTTzVu3DiFhYUpMTFRc+fO1dVXXx3wPB988IFmzZqlFStWBLXdYC9m4YrDS9DF+svUFKhV8KhV/XDFIQA0A4Q0ADiMkAYAh9XaqjQiIqIiLCyMIG9maCkZPGoVPGpVP7QqRY04wBM8ahU8alU/HDgEgGag0VqVFhQUKCwsTM8995w376GHHtK8efM0f/78Ko2U/vnPf6pDhw4qKyvzHl922WX6wx/+ELDcn/70JyUnJyslJUVJSUlaunSpJOmee+5RTEyM/H6//H6/rr/++oZ+aXDAz372M0VGRiopKana+TW139y9e7d69+4tv9+vxMTEKp+r5iaY9puvvvqqUlJSlJycrOuvv15ffvmlN2/VqlXq1auXevTooSeffDKUQw+5YGolVZ4HffbzM3DgQEnSN99842WO3+9XmzZt9Lvf/a5hB1jblS66gCsO8/PzLTIy0rp37+5dqTNhwgTLycmxw4cP2zXXXBNwCfiLL75o48eP9x6/8MILduONNwZcnrp3717z+XxWUlJiZpUtFHft2mVmZuPGjavX5b6XsvN5X13xv//7v/bFF19YYmJitfNrar9ZVlZmpaWlZlb5uenatasVFhbWub2LtVbBtN/8+OOP7bvvvjMzs7feesvS09PNzKy8vNx8Pp/t3LnTysrKLCUlpcq61WnOtSouLrb4+HjbvXu3mVmV9hZmlXXr2LGjFRQUBLVdudCqtEOHDsrMzNT8+fMDprdp00YDBw7U8uXLvWmLFi0K+Hadm5ur2bNnq7Cw0Os2dfDgQV111VXet/Urr7xSMTExjfkS4Jibb75Z7dq1q3F+Te03W7Zsqcsvv1ySVFZWpoqKoI7ZXLSCab95/fXXe1fNndtCeP369erRo4d8Pp9atmyprKws7y/W5iiYWi1cuFAjRoxQly5dJEmRkZFVnuf9999X9+7d1bVr1wYdX6Pvk548ebJmzZpVpdXh2LFjtWjRIklSUVGRtm/f7vVr3bt3r/bv36/09HSNHj3au4tGamqqOnbsqJiYGI0fPz4g5CVp0qRJ3p8dd999d2O/NDiotlale/fuVUpKijp37qzJkycHtC9ozmpqv3muc1sIFxYWqnPnzt686OjoKqHVXNVUq+3bt6u4uFi33HKL+vTpo1deeaXKut//otlQGj2kfT6f+vXrp4ULFwZMHzJkiD7++GMdOXJES5Ys0ciRIxUeHi5JWrx4sUaPHi1JysrKUm5uriQpPDxcq1at0uuvv67Y2Fg98sgjmj59uvecTz/9tPLy8pSXl6dXX321sV8aHFRbq9LOnTtr8+bN+vbbbzV//nyvaXtzVlv7zbPWrl2ruXPnej10LlW11aq8vFxffPGFVq5cqdWrV+vxxx/X9u3bvfmnTp3SsmXL9JOf/KTBxxWSszumTZump556KuD0nFatWumOO+7Qm2++We2ujnnz5qlbt24aNmyYNm/erB07dkiq/HM2PT1dU6dO1aJFi/SXv/wlFC8BF4mcnByNGDFCYWFh6tGjh2JiYvT1118HLBMVFaWkpCStW7euiUYZGqdPn9bIkSN19913a8SIEdUus3nzZt13331aunSprrnmGklSp06dtHfvXm+Zffv2qVOnTiEZc1Opq1bR0dG6/fbb1bp1a7Vv314333xzwIHWt99+W71791bHjh0bfGwhCem4uDglJCRU2T0xduxYzZkzRwcOHNCAAQMkVf5ZcezYMRUWFqqgoEAFBQWaOnWqcnNzVVRUpI0bN3rr5+XlNfj+H1zcunTpovfff1+SdODAAX3zzTfy+Xzat2+fTp48KanypqIfffSRevXq1ZRDbVRmpnvvvVfx8fF69NFHq11mz549GjFihP785z8rNjbWm963b1/t2LFD+fn5OnXqlBYtWqRhw4aFaughF0ytfvzjH+ujjz5SeXm5Tpw4oc8++0zx8fHe/Nzc3Ma79V9tRxV1gWd3nHsEPi8vz8LCwiwnJ8ebdvr0aWvfvr1NnjzZmzZ9+vSAx2ZmX375pcXFxVlBQYFlZGRYr169LDU11W699Vb79ttvzazy7I5u3bpZamqq93P2rBIEOp/31RVZWVl27bXXWosWLaxTp072P//zP/biiy/aiy++aGaVN5kdPHiwJSUlWWJiov35z382M7N33nnHkpOTLSUlxZKTk+2Pf/xjUNu7WGu1bt06k2TJycne78PKlSsDanXvvfda27Ztvfl9+vTx1l+5cqX17NnTfD6f/eY3vwlqm825VmZmM2fOtPj4eEtMTAzoS37s2DFr166dd9ZZsNRU/aThPq4MCx61Ch61qh+uOASAZoCQBgCH0QXvEkS3suBRq+BRq/qhCx5qxL7D4FGr4FGr+mGfNAA0A43WBe9cc+bMUUJCglJSUpSZmandu3dLkjZs2KDExESdOnVKkrRz5075fD7vCrHv++CDD/TDH/7Qu/T71ltvlVTZAe/111+vdhwFBQXVdkyrbp3qFBQUqFWrVkpLS1N8fLzS09M1b968Ksvddddd6t+/v/d4xowZ3jjDw8O9fz/77LPeMn6/X1lZWXWOAZUupFuZJJWUlGjUqFGKi4tTfHy8Pv3001ANPeSCqdXTTz/tfS6TkpIUHh6u7777LjSd3RwSTK2Ki4s1fPhwpaSkKD09XX//+98D5p85c0ZpaWm68847G36AtZ2fpws4T/pca9as8TrevfDCCzZ69Ghv3oMPPmgzZswwM7Pbb7/dFi5cWONzr1271oYMGVJlenUd8Go6X7u2darz/fV37txpqamp9qc//cmbVlxcbNHR0RYXF2c7d+6s8hzV1WTr1q2WlJRkUVFRduzYsTrH0ZDO5311wYV2K/v3f/93e/nll83MrKyszIqLi+vcZnOu1bmWLVtmGRkZVabXp7Nbc67VxIkTbfr06WZmtm3bNhs0aFDA/NmzZ9vYsWOrzaeayIUueGdlZGToiiuukBTYbUuSnnjiCb388suaOXOmysvLG++qnQbi8/k0Z86cgG/Eb7zxhoYOHaqsrCyvaVRdcnNzlZ2drdtuu61ZdxhrSBfSrezw4cP68MMPde+990qSWrZsqbZt24Zw9KEVTK3OVdMVc43V2c0lwdRq69atXgO4uLg4FRQUeL1f9u3bp5UrV+q+++5rlPGFfJ/0ud22JKlt27aaMmWKpk6dqueff77O9detW+f9GTZjxozGHGqNevfuHdAP4uwHfOzYsV4zqLosXrxYWVlZ9VoH/199u5Xl5+erQ4cOGj9+vNLS0nTffffp+PHjTTH0kKurC96JEye0atUqjRw5ssq8xurs5qqaapWamqo33nhDUmUr1927d3tfNn/5y19q5syZXrfFhhbSkF6wYIE2bNigSZMmBUx/++231bFjR23durXO57jpppu8Tnf/+Z//KUnVnvbTmKcC2TlHsA8cOKAdO3boxhtvVGxsrC677LIq+6u+b8OGDWrfvr26dOmizMxMbdq0Sd99912jjbe5OZ9uZeXl5dq4caMefPBBbdq0Sa1bt272dxyRguuCt3z5ct1www1V+nQ3Zmc3F9VWqylTpqikpER+v1/PPfec0tLSFB4erhUrVigyMlJ9+vRptHGFLKTfe+89zZgxQ8uWLfOar0vSihUrdPjwYa1evVqTJk3SiRMn6v3c11xzjYqLi73H3333ndq3b98g467Opk2bvOYqS5YsUXFxsWJiYtStWzcVFBTU+c04NzdXX3/9tbp166bu3bvryJEjdPML0vl2K4uOjlZ0dLT3DWnUqFEBzbqao2C64Ek1f1tuzM5urqmrVm3atFFOTo7y8vL0yiuv6NChQ/L5fPr444+1bNkydevWTVlZWVqzZo1++tOfNuzgatthrQY6cLhx40bz+Xy2ffv2gOknTpwI2En/6KOP2rRp02p87poOHC5fvtwyMzO9hkqzZ8/2bsXV0AcO8/PzLS0tzTtwOGDAAPvkk0+8+bt27TKfzxfwHOfW5MyZMxYdHR1w66Y1a9ZUe9CmsZzP++qCiooKy87OtocffrjGZbZu3WqDBg2y06dP2/Hjxy0xMdG2bNliZmY33nijff3112Zm9thjj9nEiRPr3GZzrpWZWUlJiV199dXVHrweM2ZMwAHyujTnWhUXF3v58tJLL1l2dnaVZWrKp5ooyAOHDR7SYWFh1qlTJ+9n9uzZlpmZaZGRkV6HqaFDh5qZ2dSpU7370ZlVHlmNiYmpEuZn1VaE6dOnW1JSkqWmptqIESPs4MGDZlYZqmc7pp39WbJkiY0bN87atWvnTevfv3+1z5ufn28RERHm9/stLi7O+vbt63Xyy8/Pt6ioKKuoqAhYJy0tzf72t795j88N6Q8++MD69esXsPzZI+hFRUXVjqGhXay/TBfarWzTpk3Wp08fS05Oth//+Mfe/f1q09xrlZOTY2PGjKmy/vl0dmvOtfrkk0+sZ8+eFhsba8OHD6/2s9NYIc0Vh5cgrgwLHrUKHrWqH644BIBmoNYGS01l9erVmjx5csC0mJgYvfnmm422zS1btig7Oztg2uWXX67PPvus0bYJAHWpdXdHq1atzpSWlvJtu5mJiIhQaWlpUw/jokCtgket6iciIqLi5MmT4XUtxz7pSxD7DoNHrYJHreqHfdIA0AwQ0rholJaWKj09XampqUpMTNRjjz1WZZkPP/xQvXv3VosWLap0OdyzZ49uu+02xcfHKyEhQQUFBSEaeegFU6uaulPu3r1bvXv39joJ/uEPfwj18EPqQmolheBzVdv5ebrAu4VLsmeffdabN2HCBMvJybF58+ZZVlZWwHqHDh2y9u3bW2lpqfe4RYsWAed0mpnNnTvXkpKSLDk52RITE+2vf/2rmVW9W/iAAQNqHGNOTo61b9/eW/bsiekDBw60zz//3Fvu3AtZajoH8vvr1GTt2rXWpk0b8/v9FhsbazfddJMtX768ynKpqakB563+4he/sNTUVIuPj7eIiAhvzGcvxKnujut1OZ/31QUVFRV29OhRMzM7deqUpaen26effhqwTH5+vn355ZeWnZ1d5WKlgQMH2jvvvGNmZkePHvU6M9amOdeqpu6UZWVl3u/h0aNHrWvXrgEXX9XkUqyV2fl9rsyCP0+6Uc/uiIyM1DPPPKP7779fLVu29KYPHz5cv/rVr3TixAmvO97rr7+uoUOHepeMv/baa+rfv79yc3P1wAMPSKrsNjVjxgxt3LhRP/zhD3Xs2DEdOnTIe96nn35ao0aNCmpsY8aM0e9///uGeqlBuemmm7RixQpJUl5enu666y61atVKmZmZkqRt27bpzJkzWrdunY4fP67WrVt7TacKCgp05513Ki8vL+A53333XcXGxuq1117Tb3/722Z9+6KwsDCvT/jp06d1+vTpKq+3W7duklSl2c3WrVtVXl6uwYMHS6q+73lzEkytMjIyvH/3799fCxYskKSA39WysjJVVAR1l6eL1oXUKhSfq0bd3dGhQwdlZmZq/vz5AdPbtGmjgQMHavny5d607/cPyM3N1ezZs1VYWOh1mzp48KCuuuoqrxBXXnmlYmJiGvMlNBq/369f//rXAf9RnE/70tzcXD388MPq0qVLs25if9aZM2fk9/sVGRmpwYMH19jZ7fu2b9+utm3basSIEUpLS9OkSZN05syZRh5t06pPrb7fnXLv3r1KSUlR586dNXnyZEVFRYViyE3mfGsVis9Vo++Tnjx5smbNmlVl4GPHjvV6LxcVFWn79u1ev9a9e/dq//79Sk9P1+jRo7V48WJJle0CO3bsqJiYGI0fPz4g5CVp0qRJXhvTu+++u9ZxLV682Fs2JyenoV5uvXy/5Wl925eWlpbqvffe09ChQy+Zlqfh4eHKy8vTvn37tH79+jo7Dp5VXl6udevWadasWfr888+1a9euau+w05wEW6vqulN27txZmzdv1rfffqv58+d7vZObq/OtVSg+V40e0j6fT/369dPChQsDpg8ZMkQff/yxjhw5oiVLlmjkyJEKD688ZXDx4sUaPXq0JCkrK8sLn/DwcK1atUqvv/66YmNj9cgjj2j69Onecz799NNeG9NXX3211nGNGTPGW3b8+PGSmrbl6fm0L12xYoUyMjLUqlUrjRw5Un/961+b/bfDs9q2bauMjAytWrUqqOWjo6Pl9/vl8/nUokUL3XXXXc2+C95ZtdWqpu6UZ0VFRSkpKUnr1q0LxVCbXH1rFYrPVUjO7pg2bZqeeuqpgFBq1aqV7rjjDr355pvV7uqYN2+eunXrpmHDhmnz5s3asWOHpMrQTE9P19SpU7Vo0aIGbfHZlC1Pz6d9aW5urt577z1169ZNffr00b/+9S+tWbOm0cbb1A4dOqSSkhJJ0smTJ/Xuu+8qLi4uqHX79u2rkpIS7xjGmjVrlJCQ0GhjbWrB1GrTpk26//77tWzZMu8ONlLlsZ+TJ09Kqry330cffaRevXqFbvAhdiG1CsXnKiQhHRcXp4SEhCq7J8aOHas5c+bowIEDGjBggKTKfTzHjh1TYWGhCgoKVFBQoKlTpyo3N1dFRUUB/0vl5eU16G27BBUhAAAFJ0lEQVR9brnlFi1YsMD7z2T+/PkBBwwa0ubNm/X4449rwoQJqqio0JIlS7RlyxbvNS9durTW3RdHjhzRunXrtGfPHm+d559/vlnv8ti/f78yMjKUkpKivn37avDgwbrzzjv161//WsuWLZMkff7554qOjtZrr72m+++/X4mJiZIq/wqbNWuWMjMzlZycLDPTz3/+86Z8OY0qmFpNmjRJx44d009+8hP5/X4NGzZMUuUB7H79+ik1NVUDBw7UxIkTlZyc3JQvp1FdSK1C8rmq7dQPXeApeOf2Yc7Ly7OwsDCvzadZ9aePTZ8+vcrpZF9++aXFxcVZQUGBZWRkWK9evSw1NdVuvfVW+/bbb82s6il4qampXv/X78vJybEJEyZUmV5WVmYTJkyw5ORkS0lJsZ/97Gfe6TRr1661iIiIgJann3zyiQ0cONAiIyO9aaNGjap2m98/Be/GG2+0ZcuWmVlw7Uu/X8958+ZVaTH5r3/9K+A0xpqcz/t6qaJWwaNW9SNalaImXL4bPGoVPGpVP1wWDgDNgJOtShtKTk6OnnnmmYBpN9xwQ1B3JT9fTdFmFUDzVVer0n+UlpY2/7tQXmIiIiIqaEEbHGoVPGpVPxEREQdOnjx5bV3L1RrSAICmxf96AOAwQhoAHEZIA4DDCGkAcBghDQAOI6QBwGGENAA4jJAGAIcR0gDgMEIaABxGSAOAwwhpAHAYIQ0ADiOkAcBhhDQAOIyQBgCHEdIA4DBCGgAcRkgDgMMIaQBwGCENAA4jpAHAYYQ0ADiMkAYAhxHSAOAwQhoAHEZIA4DDCGkAcBghDQAOI6QBwGGENAA4jJAGAIcR0gDgMEIaABxGSAOAwwhpAHAYIQ0ADiOkAcBhhDQAOIyQBgCHEdIA4DBCGgAcRkgDgMMIaQBwGCENAA4jpAHAYYQ0ADiMkAYAhxHSAOAwQhoAHEZIA4DDCGkAcBghDQAOI6QBwGGENAA4jJAGAIcR0gDgMEIaABxGSAOAwwhpAHAYIQ0ADiOkAcBhhDQAOIyQBgCHEdIA4DBCGgAcRkgDgMMIaQBwGCENAA4jpAHAYYQ0ADiMkAYAhxHSAOAwQhoAHEZIA4DDCGkAcBghDQAOI6QBwGGENAA4jJAGAIcR0gDgMEIaABxGSAOAwwhpAHAYIQ0ADiOkAcBhhDQAOIyQBgCHEdIA4DBCGgAcRkgDgMMIaQBwGCENAA4jpAHAYYQ0ADiMkAYAhxHSAOAwQhoAHEZIA4DDCGkAcBghDQAOI6QBwGGENAA4jJAGAIcR0gDgMEIaABxGSAOAwwhpAHAYIQ0ADiOkAcBhhDQAOIyQBgCHEdIA4DBCGgAcRkgDgMMIaQBwGCENAA4jpAHAYYQ0ADiMkAYAhxHSAOAwQhoAHEZIA4DDCGkAcBghDQAOI6QBwGGENAA4jJAGAIcR0gDgMEIaABxGSAOAwwhpAHAYIQ0ADiOkAcBhhDQAOIyQBgCHEdIA4DBCGgAcRkgDgMMIaQBwGCENAA4jpAHAYYQ0ADiMkAYAhxHSAOAwQhoAHEZIA4DDCGkAcBghDQAOI6QBwGGENAA4jJAGAIcR0gDgMEIaABxGSAOAwwhpAHAYIQ0ADiOkAcBhhDQAOIyQBgCHEdIA4DBCGgAcRkgDgMMIaQBwGCENAA4jpAHAYYQ0ADiMkAYAhxHSAOAwQhoAHEZIA4DDCGkAcBghDQAOI6QBwGGENAA4jJAGAIcR0gDgMEIaABxGSAOAwwhpAHAYIQ0ADvs/dKOZZq5t/L4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 288x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Store benchmark results as tables\n",
    "\n",
    "save_figs = False\n",
    "\n",
    "mse_table = np.zeros((len(model_mses), len(model_mses[0])))\n",
    "\n",
    "for i, model_name in enumerate(model_names) :\n",
    "    \n",
    "    for j, feature_quantile in enumerate(feature_quantiles) :\n",
    "        \n",
    "        mse_table[i, j] = np.mean(model_mses[i][j])\n",
    "\n",
    "#Plot and store mse table\n",
    "f = plt.figure(figsize = (4, 6))\n",
    "\n",
    "cells = np.round(mse_table, 3).tolist()\n",
    "\n",
    "print(\"--- MSEs ---\")\n",
    "max_len = np.max([len(model_name.upper().replace(\"\\n\", \" \")) for model_name in model_names])\n",
    "print((\"-\" * max_len) + \"   \" + \"   \".join([(str(feature_quantile) + \"0\")[:4] for feature_quantile in feature_quantiles]))\n",
    "for i in range(len(cells)) :\n",
    "    \n",
    "    curr_len = len([model_name.upper().replace(\"\\n\", \" \") for model_name in model_names][i])\n",
    "    row_str = [model_name.upper().replace(\"\\n\", \" \") for model_name in model_names][i] + (\" \" * (max_len - curr_len))\n",
    "    \n",
    "    for j in range(len(cells[i])) :\n",
    "        cells[i][j] = (str(cells[i][j]) + \"00000\")[:4]\n",
    "        \n",
    "        row_str += \"   \" + cells[i][j]\n",
    "    \n",
    "    print(row_str)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "table = plt.table(cellText=cells, rowLabels=[model_name.upper().replace(\"\\n\", \" \") for model_name in model_names], colLabels=feature_quantiles, loc='center')\n",
    "\n",
    "ax = plt.gca()\n",
    "#f.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if save_figs :\n",
    "    plt.savefig(dataset_name + \"_l2x_and_invase_full_data\" + \"_mse_table.png\", dpi=300, transparent=True)\n",
    "    plt.savefig(dataset_name + \"_l2x_and_invase_full_data\" + \"_mse_table.eps\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
