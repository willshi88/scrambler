{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#12/29/20\n",
    "#runnign synthetic benchmark graphs for all synthetic OR datasets generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#making benchmark images \n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Conv1D, MaxPooling1D, LSTM, ConvLSTM2D, GRU, CuDNNLSTM, CuDNNGRU, BatchNormalization, LocallyConnected2D, Permute, TimeDistributed, Bidirectional\n",
    "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from keras.layers.merge import _Merge\n",
    "import keras.losses\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import isolearn.keras as iso\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import isolearn.io as isoio\n",
    "import isolearn.keras as isol\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as spio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sequence_logo_helper import dna_letter_at, plot_dna_logo\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "def contain_tf_gpu_mem_usage() :\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)\n",
    "\n",
    "contain_tf_gpu_mem_usage()\n",
    "\n",
    "class EpochVariableCallback(Callback) :\n",
    "    \n",
    "    def __init__(self, my_variable, my_func) :\n",
    "        self.my_variable = my_variable       \n",
    "        self.my_func = my_func\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs={}) :\n",
    "        K.set_value(self.my_variable, self.my_func(K.get_value(self.my_variable), epoch))\n",
    "\n",
    "        \n",
    "from optimusFunctions import *\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ONLY RUN THIS CELL ONCE \n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "#Stochastic Binarized Neuron helper functions (Tensorflow)\n",
    "#ST Estimator code adopted from https://r2rt.com/beyond-binary-ternary-and-one-hot-neurons.html\n",
    "#See Github https://github.com/spitis/\n",
    "\n",
    "def st_sampled_softmax(logits):\n",
    "    with ops.name_scope(\"STSampledSoftmax\") as namescope :\n",
    "        nt_probs = tf.nn.softmax(logits)\n",
    "        onehot_dim = logits.get_shape().as_list()[1]\n",
    "        sampled_onehot = tf.one_hot(tf.squeeze(tf.multinomial(logits, 1), 1), onehot_dim, 1.0, 0.0)\n",
    "        with tf.get_default_graph().gradient_override_map({'Ceil': 'Identity', 'Mul': 'STMul'}):\n",
    "            return tf.ceil(sampled_onehot * nt_probs)\n",
    "\n",
    "def st_hardmax_softmax(logits):\n",
    "    with ops.name_scope(\"STHardmaxSoftmax\") as namescope :\n",
    "        nt_probs = tf.nn.softmax(logits)\n",
    "        onehot_dim = logits.get_shape().as_list()[1]\n",
    "        sampled_onehot = tf.one_hot(tf.argmax(nt_probs, 1), onehot_dim, 1.0, 0.0)\n",
    "        with tf.get_default_graph().gradient_override_map({'Ceil': 'Identity', 'Mul': 'STMul'}):\n",
    "            return tf.ceil(sampled_onehot * nt_probs)\n",
    "\n",
    "@ops.RegisterGradient(\"STMul\")\n",
    "def st_mul(op, grad):\n",
    "    return [grad, grad]\n",
    "\n",
    "#Gumbel Distribution Sampler\n",
    "def gumbel_softmax(logits, temperature=0.5) :\n",
    "    gumbel_dist = tf.contrib.distributions.RelaxedOneHotCategorical(temperature, logits=logits)\n",
    "    batch_dim = logits.get_shape().as_list()[0]\n",
    "    onehot_dim = logits.get_shape().as_list()[1]\n",
    "    return gumbel_dist.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PWM Masking and Sampling helper functions\n",
    "\n",
    "def mask_pwm(inputs) :\n",
    "    pwm, onehot_template, onehot_mask = inputs\n",
    "\n",
    "    return pwm * onehot_mask + onehot_template\n",
    "\n",
    "def sample_pwm_st(pwm_logits) :\n",
    "    n_sequences = K.shape(pwm_logits)[0]\n",
    "    seq_length = K.shape(pwm_logits)[2]\n",
    "\n",
    "    flat_pwm = K.reshape(pwm_logits, (n_sequences * seq_length, 4))\n",
    "    sampled_pwm = st_sampled_softmax(flat_pwm)\n",
    "\n",
    "    return K.reshape(sampled_pwm, (n_sequences, 1, seq_length, 4))\n",
    "\n",
    "def sample_pwm_gumbel(pwm_logits) :\n",
    "    n_sequences = K.shape(pwm_logits)[0]\n",
    "    seq_length = K.shape(pwm_logits)[2]\n",
    "\n",
    "    flat_pwm = K.reshape(pwm_logits, (n_sequences * seq_length, 4))\n",
    "    sampled_pwm = gumbel_softmax(flat_pwm, temperature=0.5)\n",
    "\n",
    "    return K.reshape(sampled_pwm, (n_sequences, 1, seq_length, 4))\n",
    "\n",
    "#Generator helper functions\n",
    "def initialize_sequence_templates(generator, sequence_templates, background_matrices) :\n",
    "\n",
    "    embedding_templates = []\n",
    "    embedding_masks = []\n",
    "    embedding_backgrounds = []\n",
    "\n",
    "    for k in range(len(sequence_templates)) :\n",
    "        sequence_template = sequence_templates[k]\n",
    "        onehot_template = iso.OneHotEncoder(seq_length=len(sequence_template))(sequence_template).reshape((1, len(sequence_template), 4))\n",
    "\n",
    "        for j in range(len(sequence_template)) :\n",
    "            if sequence_template[j] not in ['N', 'X'] :\n",
    "                nt_ix = np.argmax(onehot_template[0, j, :])\n",
    "                onehot_template[:, j, :] = -4.0\n",
    "                onehot_template[:, j, nt_ix] = 10.0\n",
    "            elif sequence_template[j] == 'X' :\n",
    "                onehot_template[:, j, :] = -1.0\n",
    "\n",
    "        onehot_mask = np.zeros((1, len(sequence_template), 4))\n",
    "        for j in range(len(sequence_template)) :\n",
    "            if sequence_template[j] == 'N' :\n",
    "                onehot_mask[:, j, :] = 1.0\n",
    "\n",
    "        embedding_templates.append(onehot_template.reshape(1, -1))\n",
    "        embedding_masks.append(onehot_mask.reshape(1, -1))\n",
    "        embedding_backgrounds.append(background_matrices[k].reshape(1, -1))\n",
    "\n",
    "    embedding_templates = np.concatenate(embedding_templates, axis=0)\n",
    "    embedding_masks = np.concatenate(embedding_masks, axis=0)\n",
    "    embedding_backgrounds = np.concatenate(embedding_backgrounds, axis=0)\n",
    "\n",
    "    generator.get_layer('template_dense').set_weights([embedding_templates])\n",
    "    generator.get_layer('template_dense').trainable = False\n",
    "\n",
    "    generator.get_layer('mask_dense').set_weights([embedding_masks])\n",
    "    generator.get_layer('mask_dense').trainable = False\n",
    "    \n",
    "    generator.get_layer('background_dense').set_weights([embedding_backgrounds])\n",
    "    generator.get_layer('background_dense').trainable = False\n",
    "\n",
    "#Generator construction function\n",
    "def build_sampler(batch_size, seq_length, n_classes=1, n_samples=1, sample_mode='st') :\n",
    "\n",
    "    #Initialize Reshape layer\n",
    "    reshape_layer = Reshape((1, seq_length, 4))\n",
    "    \n",
    "    #Initialize background matrix\n",
    "    onehot_background_dense = Embedding(n_classes, seq_length * 4, embeddings_initializer='zeros', name='background_dense')\n",
    "\n",
    "    #Initialize template and mask matrices\n",
    "    onehot_template_dense = Embedding(n_classes, seq_length * 4, embeddings_initializer='zeros', name='template_dense')\n",
    "    onehot_mask_dense = Embedding(n_classes, seq_length * 4, embeddings_initializer='ones', name='mask_dense')\n",
    "\n",
    "    #Initialize Templating and Masking Lambda layer\n",
    "    masking_layer = Lambda(mask_pwm, output_shape = (1, seq_length, 4), name='masking_layer')\n",
    "    background_layer = Lambda(lambda x: x[0] + x[1], name='background_layer')\n",
    "    \n",
    "    #Initialize PWM normalization layer\n",
    "    pwm_layer = Softmax(axis=-1, name='pwm')\n",
    "    \n",
    "    #Initialize sampling layers\n",
    "    sample_func = None\n",
    "    if sample_mode == 'st' :\n",
    "        sample_func = sample_pwm_st\n",
    "    elif sample_mode == 'gumbel' :\n",
    "        sample_func = sample_pwm_gumbel\n",
    "    \n",
    "    upsampling_layer = Lambda(lambda x: K.tile(x, [n_samples, 1, 1, 1]), name='upsampling_layer')\n",
    "    sampling_layer = Lambda(sample_func, name='pwm_sampler')\n",
    "    permute_layer = Lambda(lambda x: K.permute_dimensions(K.reshape(x, (n_samples, batch_size, 1, seq_length, 4)), (1, 0, 2, 3, 4)), name='permute_layer')\n",
    "    \n",
    "    def _sampler_func(class_input, raw_logits) :\n",
    "        \n",
    "        #Get Template and Mask\n",
    "        onehot_background = reshape_layer(onehot_background_dense(class_input))\n",
    "        onehot_template = reshape_layer(onehot_template_dense(class_input))\n",
    "        onehot_mask = reshape_layer(onehot_mask_dense(class_input))\n",
    "        \n",
    "        #Add Template and Multiply Mask\n",
    "        pwm_logits = masking_layer([background_layer([raw_logits, onehot_background]), onehot_template, onehot_mask])\n",
    "        \n",
    "        #Compute PWM (Nucleotide-wise Softmax)\n",
    "        pwm = pwm_layer(pwm_logits)\n",
    "        \n",
    "        #Tile each PWM to sample from and create sample axis\n",
    "        pwm_logits_upsampled = upsampling_layer(pwm_logits)\n",
    "        sampled_pwm = sampling_layer(pwm_logits_upsampled)\n",
    "        sampled_pwm = permute_layer(sampled_pwm)\n",
    "\n",
    "        sampled_mask = permute_layer(upsampling_layer(onehot_mask))\n",
    "        \n",
    "        return pwm_logits, pwm, sampled_pwm, onehot_mask, sampled_mask\n",
    "    \n",
    "    return _sampler_func\n",
    "\n",
    "#for formulation 2 graphing \n",
    "def returnXMeanLogits(e_train):\n",
    "    #returns x mean logits for displayign the pwm difference for the version 2 networks \n",
    "    #Visualize background sequence distribution\n",
    "    seq_e_train = one_hot_encode(e_train,seq_len=50)\n",
    "    x_train = seq_e_train\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1], x_train.shape[2]))\n",
    "\n",
    "    pseudo_count = 1.0\n",
    "\n",
    "    x_mean = (np.sum(x_train, axis=(0, 1)) + pseudo_count) / (x_train.shape[0] + 4. * pseudo_count)\n",
    "    x_mean_logits = np.log(x_mean / (1. - x_mean))\n",
    "    return x_mean_logits, x_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimus5_synthetic_random_insert_if_uorf_1_start_1_stop_variable_loc_512\n",
      "(512, 1, 50, 4)\n"
     ]
    }
   ],
   "source": [
    "#loading testing dataset \n",
    "\n",
    "csv_to_open = \"optimus5_synthetic_random_insert_if_uorf_1_start_1_stop_variable_loc_512.csv\"\n",
    "dataset_name = csv_to_open.replace(\".csv\", \"\")\n",
    "print (dataset_name)\n",
    "data_df = pd.read_csv(\"./\" + csv_to_open) #open from scores folder \n",
    "#loaded test set which is sorted by number of start/stop signals \n",
    "seq_e_test = one_hot_encode(data_df, seq_len=50)\n",
    "benchmarkSet_seqs = seq_e_test\n",
    "x_test = np.reshape(benchmarkSet_seqs, (benchmarkSet_seqs.shape[0], 1, benchmarkSet_seqs.shape[1], benchmarkSet_seqs.shape[2]))\n",
    "print (x_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training:  15008  testing:  512\n"
     ]
    }
   ],
   "source": [
    "#Visualize background sequence distribution\n",
    "\n",
    "e_train = pd.read_csv(\"bottom5KIFuAUGTop5KIFuAUG.csv\")\n",
    "#taking away 16 to make it divisible by batch size 32\n",
    "#e_train.drop(e_train.tail(16).index,inplace=True) # drop last 16 rows\n",
    "print (\"training: \", e_train.shape[0], \" testing: \", x_test.shape[0])\n",
    "\n",
    "#one hot encode with optimus encoders \n",
    "# One-hot encode both training and test UTRs\n",
    "seq_e_train = one_hot_encode(e_train,seq_len=50)\n",
    "\n",
    "x_mean_logits, x_mean = returnXMeanLogits(e_train)\n",
    "\n",
    "seq_e_train = one_hot_encode(e_train,seq_len=50)\n",
    "x_train = seq_e_train\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1], x_train.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training:  15008  testing:  512\n"
     ]
    }
   ],
   "source": [
    "#background \n",
    "#Visualize background sequence distribution\n",
    "\n",
    "#for formulation 2 graphing \n",
    "def returnXMeanLogits(e_train):\n",
    "    #returns x mean logits for displayign the pwm difference for the version 2 networks \n",
    "    #Visualize background sequence distribution\n",
    "    seq_e_train = one_hot_encode(e_train,seq_len=50)\n",
    "    x_train = seq_e_train\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1], x_train.shape[2]))\n",
    "\n",
    "    pseudo_count = 1.0\n",
    "\n",
    "    x_mean = (np.sum(x_train, axis=(0, 1)) + pseudo_count) / (x_train.shape[0] + 4. * pseudo_count)\n",
    "    x_mean_logits = np.log(x_mean / (1. - x_mean))\n",
    "    return x_mean_logits, x_mean\n",
    "\n",
    "\n",
    "e_train = pd.read_csv(\"bottom5KIFuAUGTop5KIFuAUG.csv\")\n",
    "print (\"training: \", e_train.shape[0], \" testing: \", x_test.shape[0])\n",
    "\n",
    "#one hot encode with optimus encoders \n",
    "seq_e_train = one_hot_encode(e_train,seq_len=50)\n",
    "x_mean_logits, x_mean = returnXMeanLogits(e_train)\n",
    "x_train = seq_e_train\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1], x_train.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+gAAAAnCAYAAACc9WIYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAAXhJREFUeJzt3LFKQ0EURdEzYm2plY3xQ/z/ysLCLj9gLTZyLZJYiGCCSA6yFrxq7jDTbgbempkAAAAA53Vx7gsAAAAAAh0AAAAqCHQAAAAoINABAACggEAHAACAAgIdAAAACgh0AAAAKCDQAQAAoIBABwAAgAICHQAAAAoIdAAAACgg0AEAAKCAQAcAAIACAh0AAAAKCHQAAAAoINABAACggEAHAACAAgIdAAAACgh0AAAAKCDQAQAAoIBABwAAgAICHQAAAAoIdAAAACgg0AEAAKDA5akb1somyUOSuyQ3Sa6SvCV5SvKS5DXJe5LZf59b9+c9z+TxV7cGAACAf+bkQE9ym+Q+ySbJdXaBvs0uwNeX2UOgf7cGAAAA7K2Z+XnqMLzW8cMAAABAZuaoB+uTAh0AAAD4G34SBwAAAAUEOgAAABQQ6AAAAFBAoAMAAEABgQ4AAAAFBDoAAAAUEOgAAABQQKADAABAAYEOAAAABQQ6AAAAFPgAbx8asfW9BQIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1008x46.8 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean conservation (bits) = 0.032049298346210106\n",
      "Mean KL Div against background (bits) = 1.9679329305814974\n"
     ]
    }
   ],
   "source": [
    "#Define sequence template for optimus\n",
    "\n",
    "sequence_template = 'N'*50\n",
    "sequence_mask = np.array([1 if sequence_template[j] == 'N' else 0 for j in range(len(sequence_template))])\n",
    "\n",
    "#Visualize background sequence distribution\n",
    "\n",
    "save_figs = True\n",
    "plot_dna_logo(np.copy(x_mean), sequence_template=sequence_template, figsize=(14, 0.65), logo_height=1.0, plot_start=0, plot_end=50)\n",
    "\n",
    "#Calculate mean training set conservation\n",
    "\n",
    "entropy = np.sum(x_mean * -np.log(x_mean), axis=-1) / np.log(2.0)\n",
    "conservation = 2.0 - entropy\n",
    "x_mean_conservation = np.sum(conservation) / np.sum(sequence_mask)\n",
    "print(\"Mean conservation (bits) = \" + str(x_mean_conservation))\n",
    "\n",
    "#Calculate mean training set kl-divergence against background\n",
    "x_train_clipped = np.clip(np.copy(x_train[:, 0, :, :]), 1e-8, 1. - 1e-8)\n",
    "kl_divs = np.sum(x_train_clipped * np.log(x_train_clipped / np.tile(np.expand_dims(x_mean, axis=0), (x_train_clipped.shape[0], 1, 1))), axis=-1) / np.log(2.0)\n",
    "x_mean_kl_divs = np.sum(kl_divs * sequence_mask, axis=-1) / np.sum(sequence_mask)\n",
    "x_mean_kl_div = np.mean(x_mean_kl_divs)\n",
    "print(\"Mean KL Div against background (bits) = \" + str(x_mean_kl_div))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initialize Encoder and Decoder networks\n",
    "batch_size = 32\n",
    "seq_length = 50\n",
    "n_samples = 128\n",
    "sample_mode = 'st'\n",
    "#sample_mode = 'gumbel'\n",
    "\n",
    "#Load sampler\n",
    "sampler = build_sampler(batch_size, seq_length, n_classes=1, n_samples=n_samples, sample_mode=sample_mode)\n",
    "\n",
    "#Load Predictor\n",
    "predictor_path = 'optimusRetrainedMain.hdf5'\n",
    "predictor = load_model(predictor_path)\n",
    "predictor.trainable = False\n",
    "predictor.compile(optimizer=keras.optimizers.SGD(lr=0.1), loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Build scrambler model\n",
    "dummy_class = Input(shape=(1,), name='dummy_class')\n",
    "input_logits = Input(shape=(1, seq_length, 4), name='input_logits')\n",
    "\n",
    "pwm_logits, pwm, sampled_pwm, pwm_mask, sampled_mask = sampler(dummy_class, input_logits)\n",
    "\n",
    "scrambler_model = Model([input_logits, dummy_class], [pwm_logits, pwm, sampled_pwm, pwm_mask, sampled_mask])\n",
    "\n",
    "#Initialize Sequence Templates and Masks\n",
    "initialize_sequence_templates(scrambler_model, [sequence_template], [x_mean_logits])\n",
    "\n",
    "scrambler_model.trainable = False\n",
    "scrambler_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999),\n",
    "    loss='mean_squared_error'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n",
      "(512, 1, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "#open all score and reshape as needed \n",
    "\n",
    "file_names = [\n",
    "    \"l2x_\" + dataset_name +  \"_importance_scores_test.npy\",\n",
    "    \"invase_\" + dataset_name +  \"_conv_importance_scores_test.npy\",\n",
    "    \"l2x_\" + dataset_name +  \"_full_data_importance_scores_test.npy\",\n",
    "    \"invase_\" + dataset_name +  \"_conv_full_data_importance_scores_test.npy\",\n",
    "]\n",
    "#deepexplain_optimus_utr_OR_logic_synth_1_start_2_stops_method_integrated_gradients_importance_scores_test.npy\n",
    "\n",
    "model_names =[\n",
    "    \"l2x\",\n",
    "    \"invase\",\n",
    "    \"l2x_full_data\",\n",
    "    \"invase_full_data\",\n",
    "]\n",
    "\n",
    "model_importance_scores_test = [np.load(\"./\" + file_name) for file_name in file_names]\n",
    "\n",
    "for scores in model_importance_scores_test:\n",
    "    print (scores.shape)\n",
    "\n",
    "for model_i in range(len(model_names)) :\n",
    "    if model_importance_scores_test[model_i].shape[-1] > 1 :\n",
    "        model_importance_scores_test[model_i] = np.sum(model_importance_scores_test[model_i], axis=-1, keepdims=True)\n",
    "\n",
    "for scores in model_importance_scores_test:\n",
    "    print (scores.shape)\n",
    "    \n",
    "#reshape for mse script -> if not (3008, 1, 50, 1) make it that shape \n",
    "idealShape = model_importance_scores_test[0].shape\n",
    "print (idealShape)\n",
    "\n",
    "for model_i in range(len(model_names)) :\n",
    "    if model_importance_scores_test[model_i].shape != idealShape:\n",
    "        model_importance_scores_test[model_i] = np.expand_dims(model_importance_scores_test[model_i], 1)\n",
    "        \n",
    "for scores in model_importance_scores_test:\n",
    "    print (scores.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 1, 50, 4)\n",
      "(512, 1, 50, 4)\n",
      "(512, 1)\n",
      "512/512 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "on_state_logit_val = 50.\n",
    "print (x_test.shape)\n",
    "\n",
    "dummy_test = np.zeros((x_test.shape[0], 1))\n",
    "x_test_logits = 2. * x_test - 1.\n",
    "\n",
    "print (x_test_logits.shape)\n",
    "print (dummy_test.shape)\n",
    "\n",
    "\n",
    "x_test_squeezed = np.squeeze(x_test)\n",
    "y_pred_ref = predictor.predict([x_test_squeezed], batch_size=32, verbose=True)[0]\n",
    "\n",
    "_, _, _, pwm_mask, sampled_mask = scrambler_model.predict([x_test_logits, dummy_test], batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'utr', 'gt', 'orig', 'l2x_0_76_quantile_MSE',\n",
      "       'l2x_0_82_quantile_MSE', 'l2x_0_88_quantile_MSE',\n",
      "       'invase_0_76_quantile_MSE', 'invase_0_82_quantile_MSE',\n",
      "       'invase_0_88_quantile_MSE', 'l2x_full_data_0_76_quantile_MSE',\n",
      "       'l2x_full_data_0_82_quantile_MSE', 'l2x_full_data_0_88_quantile_MSE',\n",
      "       'invase_full_data_0_76_quantile_MSE',\n",
      "       'invase_full_data_0_82_quantile_MSE',\n",
      "       'invase_full_data_0_88_quantile_MSE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_quantiles = [0.76, 0.82, 0.88]\n",
    "\n",
    "for name in model_names:\n",
    "    for quantile in feature_quantiles:\n",
    "        totalName = name + \"_\" + str(quantile).replace(\".\",\"_\") + \"_quantile_MSE\"\n",
    "        data_df[totalName] = None\n",
    "    \n",
    "print (data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking model 'l2x'...\n",
      "Feature quantile = 0.76\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.82\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.88\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Benchmarking model 'invase'...\n",
      "Feature quantile = 0.76\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.82\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.88\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Benchmarking model 'l2x_full_data'...\n",
      "Feature quantile = 0.76\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.82\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.88\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Benchmarking model 'invase_full_data'...\n",
      "Feature quantile = 0.76\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.82\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n",
      "Feature quantile = 0.88\n",
      "(512, 128, 1, 50, 4)\n",
      "Processing example 0...\n"
     ]
    }
   ],
   "source": [
    "#finding MSE of sequences generated using the var. methods as importnace scores in each quantile of interest\n",
    "\n",
    "feature_quantiles = [0.76, 0.82, 0.88]\n",
    "from sklearn import metrics\n",
    "model_mses = []\n",
    "for model_i in range(len(model_names)) :\n",
    "    \n",
    "    print(\"Benchmarking model '\" + str(model_names[model_i]) + \"'...\")\n",
    "    \n",
    "    feature_quantile_mses = []\n",
    "    \n",
    "    for feature_quantile_i, feature_quantile in enumerate(feature_quantiles) :\n",
    "        \n",
    "        print(\"Feature quantile = \" + str(feature_quantile))\n",
    "    \n",
    "        if len(model_importance_scores_test[model_i].shape) >= 5 :\n",
    "            importance_scores_test = np.abs(model_importance_scores_test[model_i][feature_quantile_i, ...])\n",
    "        else :\n",
    "            importance_scores_test = np.abs(model_importance_scores_test[model_i])\n",
    "        \n",
    "        n_to_test = importance_scores_test.shape[0] // batch_size * batch_size\n",
    "        importance_scores_test = importance_scores_test[:n_to_test]\n",
    "        \n",
    "        importance_scores_test *= np.expand_dims(np.max(pwm_mask[:n_to_test], axis=-1), axis=-1)\n",
    "\n",
    "        quantile_vals = np.quantile(importance_scores_test, axis=(1, 2, 3), q=feature_quantile, keepdims=True)\n",
    "        quantile_vals = np.tile(quantile_vals, (1, importance_scores_test.shape[1], importance_scores_test.shape[2], importance_scores_test.shape[3]))\n",
    "\n",
    "        top_logits_test = np.zeros(importance_scores_test.shape)\n",
    "        top_logits_test[importance_scores_test > quantile_vals] = on_state_logit_val\n",
    "        \n",
    "        top_logits_test = np.tile(top_logits_test, (1, 1, 1, 4)) * x_test_logits[:n_to_test]\n",
    "\n",
    "        _, _, samples_test, _, _ = scrambler_model.predict([top_logits_test, dummy_test[:n_to_test]], batch_size=batch_size)\n",
    "        print (samples_test.shape)\n",
    "        msesPerPoint = []\n",
    "        for data_ix in range(samples_test.shape[0]) :\n",
    "            #for each sample, look at kl divergence for the 128 size batch generated \n",
    "            #for MSE, just track the pred vs original pred \n",
    "            if data_ix % 1000 == 0 :\n",
    "                print(\"Processing example \" + str(data_ix) + \"...\")\n",
    "            \n",
    "            #from optimus R^2, MSE, Pearson R script \n",
    "            justPred = np.expand_dims(np.expand_dims(x_test[data_ix, 0, :, :], axis=0), axis=-1)\n",
    "            justPredReshape = np.reshape(justPred, (1,50,4))\n",
    "            \n",
    "            expanded = np.expand_dims(samples_test[data_ix, :, 0, :, :], axis=-1) #batch size is 128 \n",
    "            expandedReshape = np.reshape(expanded, (n_samples, 50,4))\n",
    "            \n",
    "            y_test_hat_ref = predictor.predict(x=justPredReshape, batch_size=1)[0][0]\n",
    "            \n",
    "            y_test_hat = predictor.predict(x=[expandedReshape], batch_size=32)\n",
    "            \n",
    "            pwmGenerated = y_test_hat.tolist()\n",
    "            tempOriginals = [y_test_hat_ref]*y_test_hat.shape[0]\n",
    "            \n",
    "            asArrayOrig = np.array(tempOriginals)\n",
    "            asArrayGen = np.array(pwmGenerated)\n",
    "            squeezed = np.squeeze(asArrayGen)\n",
    "            mse = metrics.mean_squared_error(asArrayOrig, squeezed)\n",
    "            #msesPerPoint.append(mse)\n",
    "            totalName = model_names[model_i] + \"_\" + str(feature_quantile).replace(\".\",\"_\") + \"_quantile_MSE\"\n",
    "            data_df.at[data_ix, totalName] = mse\n",
    "            msesPerPoint.append(mse)\n",
    "        msesPerPoint = np.array(msesPerPoint)\n",
    "        feature_quantile_mses.append(msesPerPoint)\n",
    "    model_mses.append(feature_quantile_mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MSEs ---\n",
      "----------------   0.76   0.82   0.88\n",
      "L2X                1.54   1.57   1.64\n",
      "INVASE             1.34   1.40   1.52\n",
      "L2X_FULL_DATA      1.44   1.53   1.65\n",
      "INVASE_FULL_DATA   1.75   1.78   1.81\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Store benchmark results as tables\n",
    "\n",
    "save_figs = False\n",
    "\n",
    "mse_table = np.zeros((len(model_mses), len(model_mses[0])))\n",
    "\n",
    "for i, model_name in enumerate(model_names) :\n",
    "    \n",
    "    for j, feature_quantile in enumerate(feature_quantiles) :\n",
    "        \n",
    "        mse_table[i, j] = np.mean(model_mses[i][j])\n",
    "\n",
    "#Plot and store mse table\n",
    "f = plt.figure(figsize = (4, 6))\n",
    "\n",
    "cells = np.round(mse_table, 3).tolist()\n",
    "\n",
    "print(\"--- MSEs ---\")\n",
    "max_len = np.max([len(model_name.upper().replace(\"\\n\", \" \")) for model_name in model_names])\n",
    "print((\"-\" * max_len) + \"   \" + \"   \".join([(str(feature_quantile) + \"0\")[:4] for feature_quantile in feature_quantiles]))\n",
    "for i in range(len(cells)) :\n",
    "    \n",
    "    curr_len = len([model_name.upper().replace(\"\\n\", \" \") for model_name in model_names][i])\n",
    "    row_str = [model_name.upper().replace(\"\\n\", \" \") for model_name in model_names][i] + (\" \" * (max_len - curr_len))\n",
    "    \n",
    "    for j in range(len(cells[i])) :\n",
    "        cells[i][j] = (str(cells[i][j]) + \"00000\")[:4]\n",
    "        \n",
    "        row_str += \"   \" + cells[i][j]\n",
    "    \n",
    "    print(row_str)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "table = plt.table(cellText=cells, rowLabels=[model_name.upper().replace(\"\\n\", \" \") for model_name in model_names], colLabels=feature_quantiles, loc='center')\n",
    "\n",
    "ax = plt.gca()\n",
    "#f.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if save_figs :\n",
    "    plt.savefig(dataset_name + \"_l2x_and_invase_full_data\" + \"_mse_table.png\", dpi=300, transparent=True)\n",
    "    plt.savefig(dataset_name + \"_l2x_and_invase_full_data\" + \"_mse_table.eps\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
