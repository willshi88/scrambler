{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as spio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class MySequence :\n",
    "    def __init__(self) :\n",
    "        self.dummy = 1\n",
    "\n",
    "import keras\n",
    "\n",
    "keras.utils.Sequence = MySequence\n",
    "\n",
    "import isolearn.io as isoio\n",
    "import isolearn.keras as iso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(data_df) = 34748 (loaded)\n"
     ]
    }
   ],
   "source": [
    "#Define dataset/experiment name\n",
    "dataset_name = \"apa_doubledope\"\n",
    "\n",
    "#Load cached dataframe\n",
    "data_df = pd.read_csv(\"apa_doubledope_cached_set.csv\", sep=\"\\t\")\n",
    "\n",
    "print(\"len(data_df) = \" + str(len(data_df)) + \" (loaded)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size = 31274\n",
      "Validation set size = 1737\n",
      "Test set size = 1737\n"
     ]
    }
   ],
   "source": [
    "#Make generators\n",
    "\n",
    "valid_set_size = 0.05\n",
    "test_set_size = 0.05\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "#Generate training and test set indexes\n",
    "data_index = np.arange(len(data_df), dtype=np.int)\n",
    "\n",
    "train_index = data_index[:-int(len(data_df) * (valid_set_size + test_set_size))]\n",
    "valid_index = data_index[train_index.shape[0]:-int(len(data_df) * test_set_size)]\n",
    "test_index = data_index[train_index.shape[0] + valid_index.shape[0]:]\n",
    "\n",
    "print('Training set size = ' + str(train_index.shape[0]))\n",
    "print('Validation set size = ' + str(valid_index.shape[0]))\n",
    "print('Test set size = ' + str(test_index.shape[0]))\n",
    "\n",
    "\n",
    "data_gens = {\n",
    "    gen_id : iso.DataGenerator(\n",
    "        idx,\n",
    "        {'df' : data_df},\n",
    "        batch_size=batch_size,\n",
    "        inputs = [\n",
    "            {\n",
    "                'id' : 'seq',\n",
    "                'source_type' : 'dataframe',\n",
    "                'source' : 'df',\n",
    "                'extractor' : iso.SequenceExtractor('padded_seq', start_pos=180, end_pos=180 + 205),\n",
    "                'encoder' : iso.OneHotEncoder(seq_length=205),\n",
    "                'dim' : (1, 205, 4),\n",
    "                'sparsify' : False\n",
    "            }\n",
    "        ],\n",
    "        outputs = [\n",
    "            {\n",
    "                'id' : 'hairpin',\n",
    "                'source_type' : 'dataframe',\n",
    "                'source' : 'df',\n",
    "                'extractor' : lambda row, index: row['proximal_usage'],\n",
    "                'transformer' : lambda t: t,\n",
    "                'dim' : (1,),\n",
    "                'sparsify' : False\n",
    "            }\n",
    "        ],\n",
    "        randomizers = [],\n",
    "        shuffle = True if gen_id == 'train' else False\n",
    "    ) for gen_id, idx in [('all', data_index), ('train', train_index), ('valid', valid_index), ('test', test_index)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = (31264, 1, 205, 4)\n",
      "x_test.shape = (1728, 1, 205, 4)\n",
      "y_train.shape = (31264, 1)\n",
      "y_test.shape = (1728, 1)\n"
     ]
    }
   ],
   "source": [
    "#Load data matrices\n",
    "\n",
    "x_train = np.concatenate([data_gens['train'][i][0][0] for i in range(len(data_gens['train']))], axis=0)\n",
    "x_test = np.concatenate([data_gens['test'][i][0][0] for i in range(len(data_gens['test']))], axis=0)\n",
    "\n",
    "y_train = np.concatenate([data_gens['train'][i][1][0] for i in range(len(data_gens['train']))], axis=0)\n",
    "y_test = np.concatenate([data_gens['test'][i][1][0] for i in range(len(data_gens['test']))], axis=0)\n",
    "\n",
    "print(\"x_train.shape = \" + str(x_train.shape))\n",
    "print(\"x_test.shape = \" + str(x_test.shape))\n",
    "\n",
    "print(\"y_train.shape = \" + str(y_train.shape))\n",
    "print(\"y_test.shape = \" + str(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save processed dataset\n",
    "\n",
    "save_suffix = \"\"\n",
    "\n",
    "np.savez(\n",
    "    dataset_name + save_suffix,\n",
    "    x_train=x_train,\n",
    "    y_train=y_train,\n",
    "    x_test=x_test,\n",
    "    y_test=y_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
